{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), (10000, 2))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB: throughout this laboratory the data has the following format: \n",
    "  * there are **N = 10000** data entries\n",
    "  * **y** represents the column vector containing weight information -- that which we wish to predict/the output (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,)**.\n",
    "  * **tx** represents the matrix $\\tilde{X}$ formed by laterally concatenating a column vector of 1s to the column vector of height information -- the input data (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,2)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Computing the Cost Function\n",
    "Fill in the `compute_loss` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "    \"\"\"Calculate the loss using either MSE or MAE.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2,). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute loss by MSE\n",
    "    # ***************************************************\n",
    "    e = y - np.dot(tx, w)  \n",
    "    loss = np.mean(e ** 2) \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss is:  5442.885581514151\n"
     ]
    }
   ],
   "source": [
    "print(\"The loss is: \", compute_loss(y,tx,np.array([[1],[2]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the function `grid_search()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from costs import *\n",
    "\n",
    "\n",
    "def grid_search(y, tx, grid_w0, grid_w1):\n",
    "    \"\"\"Algorithm for grid search.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        grid_w0: numpy array of shape=(num_grid_pts_w0, ). A 1D array containing num_grid_pts_w0 values of parameter w0 to be tested in the grid search.\n",
    "        grid_w1: numpy array of shape=(num_grid_pts_w1, ). A 1D array containing num_grid_pts_w1 values of parameter w1 to be tested in the grid search.\n",
    "\n",
    "    Returns:\n",
    "        losses: numpy array of shape=(num_grid_pts_w0, num_grid_pts_w1). A 2D array containing the loss value for each combination of w0 and w1\n",
    "    \"\"\"\n",
    "\n",
    "    losses = np.zeros((len(grid_w0), len(grid_w1)))\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute loss for each combination of w0 and w1.\n",
    "    # ***************************************************\n",
    "    for i, w0 in enumerate(grid_w0):\n",
    "        for j, w1 in enumerate(grid_w1):\n",
    "            losses[i, j] = compute_loss(y, tx, np.array([w0, w1]))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play with the grid search demo now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: loss*=84.84896629356496, w0*=66.66666666666669, w1*=16.666666666666686, execution time=0.213 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from grid_search import generate_w, get_best_parameters\n",
    "from plots import grid_visualization\n",
    "\n",
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=10)\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n",
    "\n",
    "# Select the best combinaison\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Print the results\n",
    "print(\n",
    "    \"Grid Search: loss*={l}, w0*={w0}, w1*={w1}, execution time={t:.3f} seconds\".format(\n",
    "        l=loss_star, w0=w0_star, w1=w1_star, t=execution_time\n",
    "    )\n",
    ")\n",
    "\n",
    "# Plot the results\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "fig.set_size_inches(10.0, 6.0)\n",
    "fig.savefig(\"grid_plot\")  # Optional saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, please fill in the functions `compute_gradient` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Computes the gradient at w.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        An numpy array of shape (2, ) (same shape as w), containing the gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute gradient vector\n",
    "    # ***************************************************\n",
    "    e = y - np.dot(tx, w)\n",
    "    N = len(y)\n",
    "    gradient = -1/N * np.dot(tx.T, e)\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `gradient_descent` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of GD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of GD\n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute gradient and loss\n",
    "        # ***************************************************\n",
    "        loss=compute_loss(y,tx,w)\n",
    "        gradient=compute_gradient(y,tx,w)\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by gradient\n",
    "        # ***************************************************\n",
    "        w = w - gamma * gradient\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\n",
    "            \"GD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your gradient descent function through gradient descent demo shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=5584.473425518335, w0=51.30574540147352, w1=9.435798704492393\n",
      "GD iter. 1/49: loss=530.604924217924, w0=66.69746902191565, w1=12.266538315840034\n",
      "GD iter. 2/49: loss=75.75675910088322, w0=71.31498610804833, w1=13.115760199244338\n",
      "GD iter. 3/49: loss=34.82042424034899, w0=72.70024123388814, w1=13.370526764265632\n",
      "GD iter. 4/49: loss=31.13615410290091, w0=73.11581777164008, w1=13.446956733772023\n",
      "GD iter. 5/49: loss=30.80456979053059, w0=73.24049073296567, w1=13.469885724623941\n",
      "GD iter. 6/49: loss=30.77472720241726, w0=73.27789262136332, w1=13.476764421879517\n",
      "GD iter. 7/49: loss=30.77204136948706, w0=73.28911318788263, w1=13.478828031056189\n",
      "GD iter. 8/49: loss=30.771799644523348, w0=73.29247935783842, w1=13.47944711380919\n",
      "GD iter. 9/49: loss=30.77177788927661, w0=73.29348920882516, w1=13.47963283863509\n",
      "GD iter. 10/49: loss=30.7717759313044, w0=73.29379216412119, w1=13.479688556082861\n",
      "GD iter. 11/49: loss=30.771775755086907, w0=73.29388305071, w1=13.479705271317192\n",
      "GD iter. 12/49: loss=30.771775739227333, w0=73.29391031668663, w1=13.479710285887492\n",
      "GD iter. 13/49: loss=30.771775737799967, w0=73.29391849647962, w1=13.479711790258582\n",
      "GD iter. 14/49: loss=30.7717757376715, w0=73.29392095041752, w1=13.479712241569908\n",
      "GD iter. 15/49: loss=30.771775737659947, w0=73.29392168659889, w1=13.479712376963306\n",
      "GD iter. 16/49: loss=30.7717757376589, w0=73.2939219074533, w1=13.479712417581325\n",
      "GD iter. 17/49: loss=30.771775737658807, w0=73.29392197370962, w1=13.479712429766732\n",
      "GD iter. 18/49: loss=30.7717757376588, w0=73.29392199358651, w1=13.479712433422353\n",
      "GD iter. 19/49: loss=30.771775737658796, w0=73.29392199954958, w1=13.47971243451904\n",
      "GD iter. 20/49: loss=30.771775737658796, w0=73.29392200133852, w1=13.479712434848047\n",
      "GD iter. 21/49: loss=30.7717757376588, w0=73.29392200187519, w1=13.479712434946748\n",
      "GD iter. 22/49: loss=30.7717757376588, w0=73.29392200203618, w1=13.479712434976358\n",
      "GD iter. 23/49: loss=30.7717757376588, w0=73.29392200208449, w1=13.479712434985242\n",
      "GD iter. 24/49: loss=30.7717757376588, w0=73.29392200209898, w1=13.479712434987906\n",
      "GD iter. 25/49: loss=30.771775737658796, w0=73.29392200210333, w1=13.479712434988706\n",
      "GD iter. 26/49: loss=30.7717757376588, w0=73.29392200210462, w1=13.479712434988945\n",
      "GD iter. 27/49: loss=30.7717757376588, w0=73.29392200210502, w1=13.479712434989018\n",
      "GD iter. 28/49: loss=30.7717757376588, w0=73.29392200210515, w1=13.47971243498904\n",
      "GD iter. 29/49: loss=30.7717757376588, w0=73.29392200210518, w1=13.479712434989047\n",
      "GD iter. 30/49: loss=30.7717757376588, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 31/49: loss=30.7717757376588, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 32/49: loss=30.7717757376588, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 33/49: loss=30.7717757376588, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 34/49: loss=30.7717757376588, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 35/49: loss=30.7717757376588, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 36/49: loss=30.7717757376588, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 37/49: loss=30.7717757376588, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 38/49: loss=30.7717757376588, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 39/49: loss=30.7717757376588, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 40/49: loss=30.7717757376588, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 41/49: loss=30.7717757376588, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 42/49: loss=30.7717757376588, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 43/49: loss=30.7717757376588, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 44/49: loss=30.7717757376588, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 45/49: loss=30.7717757376588, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 46/49: loss=30.7717757376588, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 47/49: loss=30.7717757376588, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 48/49: loss=30.7717757376588, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 49/49: loss=30.7717757376588, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD: execution time=0.110 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50e871f5e85e44cab57dc32f72f67899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses,\n",
    "        gd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient at w from a data sample batch of size B, where B < N, and their corresponding labels.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(B, )\n",
    "        tx: numpy array of shape=(B,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the stochastic gradient of the loss at w.\n",
    "    \"\"\"\n",
    "\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: implement stochastic gradient computation. It's the same as the usual gradient.\n",
    "    # ***************************************************\n",
    "    e = y - np.dot(tx, w)  # Erreur pour le batch\n",
    "    N = len(y)  # Taille du batch\n",
    "    gradient = -1 / N * np.dot(tx.T, e)  # Calcul du gradient pour le batch\n",
    "    return gradient\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD).\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic gradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SGD\n",
    "    \"\"\"\n",
    "\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: implement stochastic gradient descent.\n",
    "        # ***************************************************\n",
    "        # Obtenir un mini-batch\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size):\n",
    "            # Calculer le gradient stochastique pour ce mini-batch\n",
    "            gradient = compute_stoch_gradient(minibatch_y, minibatch_tx, w)\n",
    "\n",
    "            # Mettre à jour les poids avec le gradient stochastique\n",
    "            w = w - gamma * gradient\n",
    "\n",
    "            # Calculer la perte pour ce mini-batch\n",
    "            loss = compute_loss(minibatch_y, minibatch_tx, w)\n",
    "            \n",
    "            # Stocker les poids et les pertes\n",
    "        ws.append(w.copy())\n",
    "        losses.append(loss)\n",
    "        \n",
    "        print(\n",
    "            \"SGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 0/49: loss=1437.8028748369152, w0=4.648976963552144, w1=-4.270274820369571\n",
      "SGD iter. 1/49: loss=2957.4661703811303, w0=14.586413014389237, w1=14.393867273469201\n",
      "SGD iter. 2/49: loss=3433.8473545513802, w0=21.127139158674687, w1=15.71659966965257\n",
      "SGD iter. 3/49: loss=1834.5346879760261, w0=26.077793495386718, w1=18.638415939989272\n",
      "SGD iter. 4/49: loss=2111.630817791072, w0=31.309571733189625, w1=16.203190246356154\n",
      "SGD iter. 5/49: loss=983.2913798623703, w0=35.792775482972786, w1=9.85416802709599\n",
      "SGD iter. 6/49: loss=296.9995802004431, w0=38.118523286935456, w1=6.921465847239535\n",
      "SGD iter. 7/49: loss=785.6451180126404, w0=41.50922498794443, w1=4.017586064929866\n",
      "SGD iter. 8/49: loss=753.9078742111584, w0=46.10527673661583, w1=12.012438941797061\n",
      "SGD iter. 9/49: loss=213.83210255039452, w0=48.66829709371299, w1=16.66460142758496\n",
      "SGD iter. 10/49: loss=461.7411990659114, w0=51.79889449432528, w1=12.089126600634145\n",
      "SGD iter. 11/49: loss=646.1092726019145, w0=54.930684072096156, w1=15.033103126232255\n",
      "SGD iter. 12/49: loss=138.6538371749408, w0=56.25203065636845, w1=15.426263276812394\n",
      "SGD iter. 13/49: loss=302.1622165870308, w0=58.51673504855357, w1=12.819917361453825\n",
      "SGD iter. 14/49: loss=109.23240882600612, w0=59.93806684971736, w1=10.995991477734199\n",
      "SGD iter. 15/49: loss=28.90600926035433, w0=60.589332000004724, w1=10.433999855539485\n",
      "SGD iter. 16/49: loss=19.352079397702784, w0=61.24290922588505, w1=9.44946075410793\n",
      "SGD iter. 17/49: loss=0.48843208564984286, w0=61.34352858530622, w1=9.305247281663293\n",
      "SGD iter. 18/49: loss=37.08908586683635, w0=62.0402539622713, w1=8.950673849408739\n",
      "SGD iter. 19/49: loss=33.96552384637411, w0=62.78236507580311, w1=8.155978685443191\n",
      "SGD iter. 20/49: loss=37.354027498682626, w0=61.95276891360314, w1=9.216048105749818\n",
      "SGD iter. 21/49: loss=218.98663645943108, w0=63.635951222509306, w1=9.984071970477498\n",
      "SGD iter. 22/49: loss=275.43280552831595, w0=65.64485181595217, w1=11.710652928140833\n",
      "SGD iter. 23/49: loss=96.8561222732387, w0=66.75171786347605, w1=11.34583299726917\n",
      "SGD iter. 24/49: loss=0.08454337760963693, w0=66.78438972148699, w1=11.335475828218318\n",
      "SGD iter. 25/49: loss=126.6824880538445, w0=68.17546784965892, w1=12.66168836729873\n",
      "SGD iter. 26/49: loss=12.578271391448762, w0=68.61852140192242, w1=12.219714861513987\n",
      "SGD iter. 27/49: loss=10.37118495851853, w0=68.97683720578635, w1=12.259465422066867\n",
      "SGD iter. 28/49: loss=32.5706763544998, w0=69.68153020623329, w1=12.928491572789333\n",
      "SGD iter. 29/49: loss=6.44504313923257, w0=70.1232639176065, w1=12.131795445816644\n",
      "SGD iter. 30/49: loss=111.93928911410586, w0=68.9460330253794, w1=12.264421192617991\n",
      "SGD iter. 31/49: loss=19.13984424998159, w0=69.48563627420728, w1=11.754687016593323\n",
      "SGD iter. 32/49: loss=4.761188511332766, w0=69.24310886021993, w1=11.768010541683351\n",
      "SGD iter. 33/49: loss=50.036333435157374, w0=70.0631174521763, w1=12.26929300268563\n",
      "SGD iter. 34/49: loss=74.99182236347797, w0=69.10041561930811, w1=12.33536473929387\n",
      "SGD iter. 35/49: loss=21.125213494683315, w0=69.63481723716633, w1=11.99766688636349\n",
      "SGD iter. 36/49: loss=15.566084035416537, w0=70.11870947505474, w1=12.442889079960155\n",
      "SGD iter. 37/49: loss=28.767399212539434, w0=70.74595867924627, w1=12.02252333391005\n",
      "SGD iter. 38/49: loss=3.0615903407354996, w0=70.54025651332871, w1=12.167074728648632\n",
      "SGD iter. 39/49: loss=67.51882206973487, w0=71.45567396727758, w1=12.025895110468053\n",
      "SGD iter. 40/49: loss=20.61388686602583, w0=71.96287956674648, w1=11.914196768413843\n",
      "SGD iter. 41/49: loss=40.124525633975054, w0=72.6749544110203, w1=11.684210129786486\n",
      "SGD iter. 42/49: loss=18.294324164067568, w0=72.12388512027736, w1=12.297455716985594\n",
      "SGD iter. 43/49: loss=15.265526762925813, w0=72.55825224165059, w1=12.266600744912408\n",
      "SGD iter. 44/49: loss=22.89785128315812, w0=73.11079595569385, w1=12.588665080895058\n",
      "SGD iter. 45/49: loss=4.497982359739614, w0=73.3817745867014, w1=12.882197302680645\n",
      "SGD iter. 46/49: loss=11.68723950538712, w0=73.00165418333222, w1=12.912546723242375\n",
      "SGD iter. 47/49: loss=19.607098387065193, w0=73.53875322378514, w1=12.445632325910438\n",
      "SGD iter. 48/49: loss=8.297937884442819, w0=73.21023308453637, w1=12.603713239249869\n",
      "SGD iter. 49/49: loss=0.5422841977145206, w0=73.31624931536766, w1=12.451776827273202\n",
      "SGD: execution time=0.009 seconds\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma\n",
    ")\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9fab909c0aa40b69080a8f3eac2ac45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_losses,\n",
    "        sgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(sgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Effect of Outliers and MAE Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: reload the data by subsampling first, then by subsampling and adding outliers\n",
    "# ***************************************************\n",
    "height, weight, gender = load_data(sub_sample=True, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)\n",
    "\n",
    "#With outliers\n",
    "# height, weight, gender = load_data(sub_sample=True, add_outlier=True)\n",
    "# x, mean_x, std_x_wo = standardize(height)\n",
    "# y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200,), (200, 2))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=5658.544448876833, w0=51.54259072181176, w1=10.132993413506084\n",
      "GD iter. 1/49: loss=534.1000517558858, w0=67.0053679383553, w1=13.172891437557825\n",
      "GD iter. 2/49: loss=72.90005601500091, w0=71.64420110331838, w1=14.084860844773322\n",
      "GD iter. 3/49: loss=31.39205639832127, w0=73.03585105280729, w1=14.358451666937965\n",
      "GD iter. 4/49: loss=27.656336432820154, w0=73.45334603765397, w1=14.440528913587356\n",
      "GD iter. 5/49: loss=27.320121635925045, w0=73.57859453310797, w1=14.46515208758217\n",
      "GD iter. 6/49: loss=27.289862304204483, w0=73.61616908174418, w1=14.472539039780616\n",
      "GD iter. 7/49: loss=27.287138964349634, w0=73.62744144633503, w1=14.474755125440149\n",
      "GD iter. 8/49: loss=27.286893863762707, w0=73.63082315571229, w1=14.47541995113801\n",
      "GD iter. 9/49: loss=27.286871804709882, w0=73.63183766852546, w1=14.475619398847368\n",
      "GD iter. 10/49: loss=27.28686981939512, w0=73.63214202236942, w1=14.475679233160175\n",
      "GD iter. 11/49: loss=27.286869640716795, w0=73.6322333285226, w1=14.475697183454017\n",
      "GD iter. 12/49: loss=27.286869624635752, w0=73.63226072036856, w1=14.47570256854217\n",
      "GD iter. 13/49: loss=27.28686962318846, w0=73.63226893792235, w1=14.475704184068615\n",
      "GD iter. 14/49: loss=27.28686962305819, w0=73.63227140318848, w1=14.475704668726548\n",
      "GD iter. 15/49: loss=27.286869623046467, w0=73.63227214276833, w1=14.47570481412393\n",
      "GD iter. 16/49: loss=27.286869623045412, w0=73.63227236464228, w1=14.475704857743143\n",
      "GD iter. 17/49: loss=27.28686962304532, w0=73.63227243120446, w1=14.475704870828908\n",
      "GD iter. 18/49: loss=27.286869623045312, w0=73.63227245117312, w1=14.475704874754637\n",
      "GD iter. 19/49: loss=27.286869623045312, w0=73.63227245716372, w1=14.475704875932356\n",
      "GD iter. 20/49: loss=27.286869623045305, w0=73.6322724589609, w1=14.475704876285672\n",
      "GD iter. 21/49: loss=27.286869623045312, w0=73.63227245950004, w1=14.475704876391665\n",
      "GD iter. 22/49: loss=27.286869623045312, w0=73.63227245966179, w1=14.475704876423464\n",
      "GD iter. 23/49: loss=27.28686962304531, w0=73.63227245971032, w1=14.475704876433003\n",
      "GD iter. 24/49: loss=27.286869623045312, w0=73.63227245972487, w1=14.475704876435865\n",
      "GD iter. 25/49: loss=27.286869623045312, w0=73.63227245972924, w1=14.475704876436724\n",
      "GD iter. 26/49: loss=27.28686962304531, w0=73.63227245973054, w1=14.475704876436982\n",
      "GD iter. 27/49: loss=27.28686962304531, w0=73.63227245973094, w1=14.47570487643706\n",
      "GD iter. 28/49: loss=27.286869623045312, w0=73.63227245973106, w1=14.475704876437083\n",
      "GD iter. 29/49: loss=27.286869623045312, w0=73.6322724597311, w1=14.475704876437089\n",
      "GD iter. 30/49: loss=27.286869623045312, w0=73.63227245973111, w1=14.47570487643709\n",
      "GD iter. 31/49: loss=27.286869623045312, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 32/49: loss=27.28686962304531, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 33/49: loss=27.28686962304531, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 34/49: loss=27.28686962304531, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 35/49: loss=27.28686962304531, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 36/49: loss=27.28686962304531, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 37/49: loss=27.28686962304531, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 38/49: loss=27.28686962304531, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 39/49: loss=27.28686962304531, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 40/49: loss=27.28686962304531, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 41/49: loss=27.28686962304531, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 42/49: loss=27.28686962304531, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 43/49: loss=27.28686962304531, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 44/49: loss=27.28686962304531, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 45/49: loss=27.28686962304531, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 46/49: loss=27.28686962304531, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 47/49: loss=27.28686962304531, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 48/49: loss=27.28686962304531, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 49/49: loss=27.28686962304531, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD: execution time=0.006 seconds\n"
     ]
    }
   ],
   "source": [
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: fit the model to the subsampled data / subsampled data with outliers and visualize the cloud of points\n",
    "#       and the model fit\n",
    "# ***************************************************\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "405616f7606b430dbaa16636f4e0b8ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses,\n",
    "        gd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 6. Subgradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_subgradient_mae(y, tx, w):\n",
    "    \"\"\"Compute a subgradient of the MAE at w.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the subgradient of the MAE at w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute subgradient gradient vector for MAE\n",
    "    # ***************************************************\n",
    "    e = y - np.dot(tx,w)\n",
    "    subgradient = -np.dot(tx.T, np.sign(e)) / len(y)\n",
    "\n",
    "    return subgradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_MAE(y, tx, w):\n",
    "    \"\"\"Calculate the loss using MAE.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2,). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute loss by MSE\n",
    "    # ***************************************************\n",
    "    e = np.abs(y - np.dot(tx, w))\n",
    "    loss = np.mean(e) \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The SubGradient Descent (SubGD) algorithm.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubGD\n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute subgradient and loss\n",
    "        # ***************************************************\n",
    "        subgradient = compute_subgradient_mae(y, tx, w)\n",
    "        loss = compute_loss_MAE(y, tx, w)\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by subgradient\n",
    "        # ***************************************************\n",
    "        w = w - gamma * subgradient\n",
    "\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\n",
    "            \"SubGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubGD iter. 0/499: loss=73.29392200210518, w0=0.7, w1=-1.846274244599044e-15\n",
      "SubGD iter. 1/499: loss=72.59392200210517, w0=1.4, w1=-3.692548489198088e-15\n",
      "SubGD iter. 2/499: loss=71.89392200210517, w0=2.0999999999999996, w1=-5.538822733797132e-15\n",
      "SubGD iter. 3/499: loss=71.19392200210518, w0=2.8, w1=-7.385096978396176e-15\n",
      "SubGD iter. 4/499: loss=70.49392200210517, w0=3.5, w1=-9.231371222995221e-15\n",
      "SubGD iter. 5/499: loss=69.79392200210518, w0=4.2, w1=-1.1077645467594266e-14\n",
      "SubGD iter. 6/499: loss=69.09392200210519, w0=4.9, w1=-1.292391971219331e-14\n",
      "SubGD iter. 7/499: loss=68.39392200210517, w0=5.6000000000000005, w1=-1.4770193956792355e-14\n",
      "SubGD iter. 8/499: loss=67.69392200210518, w0=6.300000000000001, w1=-1.66164682013914e-14\n",
      "SubGD iter. 9/499: loss=66.99392200210517, w0=7.000000000000001, w1=-1.8462742445990445e-14\n",
      "SubGD iter. 10/499: loss=66.29392200210518, w0=7.700000000000001, w1=-2.030901669058949e-14\n",
      "SubGD iter. 11/499: loss=65.59392200210517, w0=8.4, w1=-2.2155290935188535e-14\n",
      "SubGD iter. 12/499: loss=64.89392200210519, w0=9.1, w1=-2.400156517978758e-14\n",
      "SubGD iter. 13/499: loss=64.19392200210517, w0=9.799999999999999, w1=-2.5847839424386624e-14\n",
      "SubGD iter. 14/499: loss=63.49392200210517, w0=10.499999999999998, w1=-2.769411366898567e-14\n",
      "SubGD iter. 15/499: loss=62.793922002105184, w0=11.199999999999998, w1=-2.954038791358471e-14\n",
      "SubGD iter. 16/499: loss=62.09392200210518, w0=11.899999999999997, w1=-3.138666215818375e-14\n",
      "SubGD iter. 17/499: loss=61.393922002105185, w0=12.599999999999996, w1=-3.3232936402782794e-14\n",
      "SubGD iter. 18/499: loss=60.69392200210518, w0=13.299999999999995, w1=-3.5079210647381836e-14\n",
      "SubGD iter. 19/499: loss=59.99392200210518, w0=13.999999999999995, w1=-3.692548489198088e-14\n",
      "SubGD iter. 20/499: loss=59.29392200210517, w0=14.699999999999994, w1=-3.877175913657992e-14\n",
      "SubGD iter. 21/499: loss=58.593922002105174, w0=15.399999999999993, w1=-4.061803338117896e-14\n",
      "SubGD iter. 22/499: loss=57.893922002105185, w0=16.099999999999994, w1=-4.2464307625778e-14\n",
      "SubGD iter. 23/499: loss=57.19392200210517, w0=16.799999999999994, w1=-4.4310581870377044e-14\n",
      "SubGD iter. 24/499: loss=56.49392200210518, w0=17.499999999999993, w1=-4.6156856114976086e-14\n",
      "SubGD iter. 25/499: loss=55.793922002105184, w0=18.199999999999992, w1=-4.800313035957513e-14\n",
      "SubGD iter. 26/499: loss=55.09392200210518, w0=18.89999999999999, w1=-4.984940460417417e-14\n",
      "SubGD iter. 27/499: loss=54.393922002105185, w0=19.59999999999999, w1=-5.169567884877321e-14\n",
      "SubGD iter. 28/499: loss=53.69392200210518, w0=20.29999999999999, w1=-5.354195309337225e-14\n",
      "SubGD iter. 29/499: loss=52.99392200210518, w0=20.99999999999999, w1=-5.5388227337971294e-14\n",
      "SubGD iter. 30/499: loss=52.29392200210519, w0=21.69999999999999, w1=-5.723450158257034e-14\n",
      "SubGD iter. 31/499: loss=51.59392200210518, w0=22.399999999999988, w1=-5.908077582716938e-14\n",
      "SubGD iter. 32/499: loss=50.893922002105185, w0=23.099999999999987, w1=-6.092705007176843e-14\n",
      "SubGD iter. 33/499: loss=50.1939220021052, w0=23.799999999999986, w1=-6.277332431636748e-14\n",
      "SubGD iter. 34/499: loss=49.49392200210519, w0=24.499999999999986, w1=-6.461959856096653e-14\n",
      "SubGD iter. 35/499: loss=48.79392200210519, w0=25.199999999999985, w1=-6.646587280556558e-14\n",
      "SubGD iter. 36/499: loss=48.09392200210519, w0=25.899999999999984, w1=-6.831214705016462e-14\n",
      "SubGD iter. 37/499: loss=47.393922002105185, w0=26.599999999999984, w1=-7.015842129476367e-14\n",
      "SubGD iter. 38/499: loss=46.6939220021052, w0=27.299999999999983, w1=-7.200469553936272e-14\n",
      "SubGD iter. 39/499: loss=45.993922002105194, w0=27.999999999999982, w1=-7.385096978396177e-14\n",
      "SubGD iter. 40/499: loss=45.29392200210519, w0=28.69999999999998, w1=-7.569724402856082e-14\n",
      "SubGD iter. 41/499: loss=44.59392200210519, w0=29.39999999999998, w1=-7.754351827315986e-14\n",
      "SubGD iter. 42/499: loss=43.89392723059968, w0=30.099859999999982, w1=0.0004404657702295935\n",
      "SubGD iter. 43/499: loss=43.194206925442394, w0=30.799719999999983, w1=0.0008809315405367306\n",
      "SubGD iter. 44/499: loss=42.494486620285116, w0=31.499579999999984, w1=0.0013213973108438676\n",
      "SubGD iter. 45/499: loss=41.794801882440076, w0=32.19929999999999, w1=0.0021512000587904454\n",
      "SubGD iter. 46/499: loss=41.09536078676493, w0=32.899019999999986, w1=0.0029810028067370235\n",
      "SubGD iter. 47/499: loss=40.396015121762325, w0=33.59859999999998, w1=0.004238399708358765\n",
      "SubGD iter. 48/499: loss=39.696964631836686, w0=34.298039999999986, w1=0.00582382240877786\n",
      "SubGD iter. 49/499: loss=38.99808059302934, w0=34.99747999999999, w1=0.007409245109196954\n",
      "SubGD iter. 50/499: loss=38.29919655422201, w0=35.69691999999999, w1=0.008994667809616047\n",
      "SubGD iter. 51/499: loss=37.600470352601796, w0=36.39607999999999, w1=0.011248049853473857\n",
      "SubGD iter. 52/499: loss=36.90236166793368, w0=37.09495999999999, w1=0.014269124444682564\n",
      "SubGD iter. 53/499: loss=36.20468598163629, w0=37.793699999999994, w1=0.017663499462324806\n",
      "SubGD iter. 54/499: loss=35.5072889013021, w0=38.49215999999999, w1=0.021587972991141492\n",
      "SubGD iter. 55/499: loss=34.81047272559701, w0=39.19019999999999, w1=0.026549981650498426\n",
      "SubGD iter. 56/499: loss=34.114639385110515, w0=39.88739999999999, w1=0.033459020630888514\n",
      "SubGD iter. 57/499: loss=33.42034933030704, w0=40.584039999999995, w1=0.04155352991356979\n",
      "SubGD iter. 58/499: loss=32.72708260590542, w0=41.28025999999999, w1=0.05080738787868853\n",
      "SubGD iter. 59/499: loss=32.035274191106154, w0=41.97507999999999, w1=0.06316953459262972\n",
      "SubGD iter. 60/499: loss=31.346253501120465, w0=42.667939999999994, w1=0.07970663453185631\n",
      "SubGD iter. 61/499: loss=30.660811185738613, w0=43.35925999999999, w1=0.09929003796956086\n",
      "SubGD iter. 62/499: loss=29.978243733924085, w0=44.04889999999999, w1=0.12249682042555231\n",
      "SubGD iter. 63/499: loss=29.299192170025577, w0=44.735739999999986, w1=0.1511767351514394\n",
      "SubGD iter. 64/499: loss=28.625370150150545, w0=45.41991999999998, w1=0.18484712944078804\n",
      "SubGD iter. 65/499: loss=27.95640812788488, w0=46.10073999999998, w1=0.22490170464880502\n",
      "SubGD iter. 66/499: loss=27.29321133570228, w0=46.77847999999998, w1=0.2703080759889886\n",
      "SubGD iter. 67/499: loss=26.635946775492176, w0=47.45243999999998, w1=0.3220564131364331\n",
      "SubGD iter. 68/499: loss=25.98635445566157, w0=48.119959999999985, w1=0.38436028435163555\n",
      "SubGD iter. 69/499: loss=25.34635990054962, w0=48.78257999999998, w1=0.4546014318706369\n",
      "SubGD iter. 70/499: loss=24.713407766391825, w0=49.44141999999998, w1=0.5311991534378508\n",
      "SubGD iter. 71/499: loss=24.08693114448955, w0=50.094659999999976, w1=0.6167864550871617\n",
      "SubGD iter. 72/499: loss=23.469061835298348, w0=50.74159999999998, w1=0.7118294204256878\n",
      "SubGD iter. 73/499: loss=22.860312219449895, w0=51.382239999999975, w1=0.8163783816188424\n",
      "SubGD iter. 74/499: loss=22.260394194603617, w0=52.015879999999974, w1=0.9309819479179974\n",
      "SubGD iter. 75/499: loss=21.67021435798937, w0=52.64223999999997, w1=1.0560410394455484\n",
      "SubGD iter. 76/499: loss=21.089713092061306, w0=53.261179999999975, w1=1.1909782249707246\n",
      "SubGD iter. 77/499: loss=20.51839520029387, w0=53.87353999999998, w1=1.3354326069268594\n",
      "SubGD iter. 78/499: loss=19.954842019021978, w0=54.479039999999976, w1=1.4888591554230888\n",
      "SubGD iter. 79/499: loss=19.39981727796524, w0=55.07641999999998, w1=1.6526057998941222\n",
      "SubGD iter. 80/499: loss=18.854292127308746, w0=55.664699999999975, w1=1.827199911738525\n",
      "SubGD iter. 81/499: loss=18.318783947893174, w0=56.24345999999998, w1=2.013212915976202\n",
      "SubGD iter. 82/499: loss=17.792491479525175, w0=56.81479999999998, w1=2.207598649785994\n",
      "SubGD iter. 83/499: loss=17.274744345404276, w0=57.37633999999998, w1=2.411376270974451\n",
      "SubGD iter. 84/499: loss=16.76690675267884, w0=57.92975999999998, w1=2.6230481013704634\n",
      "SubGD iter. 85/499: loss=16.2679944797493, w0=58.47407999999998, w1=2.8433267790650953\n",
      "SubGD iter. 86/499: loss=15.777334186667384, w0=59.00957999999998, w1=3.07233096552034\n",
      "SubGD iter. 87/499: loss=15.2949068347808, w0=59.53569999999998, w1=3.3102924945408865\n",
      "SubGD iter. 88/499: loss=14.820404573025092, w0=60.053559999999976, w1=3.5553517483538095\n",
      "SubGD iter. 89/499: loss=14.354283714893954, w0=60.559939999999976, w1=3.809170691886047\n",
      "SubGD iter. 90/499: loss=13.898001392894924, w0=61.057499999999976, w1=4.069720751433967\n",
      "SubGD iter. 91/499: loss=13.448823828103789, w0=61.547919999999976, w1=4.335508393726889\n",
      "SubGD iter. 92/499: loss=13.006044196344108, w0=62.02965999999998, w1=4.606989747001021\n",
      "SubGD iter. 93/499: loss=12.571101749360688, w0=62.502719999999975, w1=4.885020142516802\n",
      "SubGD iter. 94/499: loss=12.143380485959396, w0=62.96583999999997, w1=5.168754626653861\n",
      "SubGD iter. 95/499: loss=11.72389483724559, w0=63.41971999999997, w1=5.456484559032027\n",
      "SubGD iter. 96/499: loss=11.313435023601558, w0=63.86463999999997, w1=5.7479974967700915\n",
      "SubGD iter. 97/499: loss=10.91115692001511, w0=64.30129999999997, w1=6.04322122396836\n",
      "SubGD iter. 98/499: loss=10.516948122608088, w0=64.72773999999997, w1=6.340354080836899\n",
      "SubGD iter. 99/499: loss=10.13359513103868, w0=65.14381999999996, w1=6.639945245906315\n",
      "SubGD iter. 100/499: loss=9.760677429923499, w0=65.54995999999996, w1=6.939754306256967\n",
      "SubGD iter. 101/499: loss=9.39867462318995, w0=65.94755999999995, w1=7.240382293155331\n",
      "SubGD iter. 102/499: loss=9.046552477133668, w0=66.33451999999996, w1=7.5427595079228436\n",
      "SubGD iter. 103/499: loss=8.704760514732428, w0=66.71139999999995, w1=7.846017516367548\n",
      "SubGD iter. 104/499: loss=8.372871671628282, w0=67.07791999999995, w1=8.149168076773316\n",
      "SubGD iter. 105/499: loss=8.052713872262608, w0=67.43603999999995, w1=8.450471378640364\n",
      "SubGD iter. 106/499: loss=7.741845358498831, w0=67.78645999999995, w1=8.75104545266106\n",
      "SubGD iter. 107/499: loss=7.4406364978479616, w0=68.12511999999995, w1=9.049215722252844\n",
      "SubGD iter. 108/499: loss=7.152144892432394, w0=68.45537999999995, w1=9.3439760876996\n",
      "SubGD iter. 109/499: loss=6.874953545674719, w0=68.77695999999995, w1=9.635092372672965\n",
      "SubGD iter. 110/499: loss=6.609759691657269, w0=69.08803999999995, w1=9.919493282073326\n",
      "SubGD iter. 111/499: loss=6.3596644653474215, w0=69.38749999999995, w1=10.197623582278961\n",
      "SubGD iter. 112/499: loss=6.1255598303371, w0=69.67281999999994, w1=10.468063593613348\n",
      "SubGD iter. 113/499: loss=5.90875685112612, w0=69.94651999999995, w1=10.728826346301961\n",
      "SubGD iter. 114/499: loss=5.708346277966684, w0=70.20887999999995, w1=10.979299692022233\n",
      "SubGD iter. 115/499: loss=5.524903595719362, w0=70.45653999999995, w1=11.220392679253326\n",
      "SubGD iter. 116/499: loss=5.358572110476793, w0=70.69215999999994, w1=11.449066352016674\n",
      "SubGD iter. 117/499: loss=5.20836657873334, w0=70.91545999999994, w1=11.664512641643359\n",
      "SubGD iter. 118/499: loss=5.0754520509180505, w0=71.12447999999993, w1=11.86438716830889\n",
      "SubGD iter. 119/499: loss=4.960839914882415, w0=71.31865999999994, w1=12.045105403559429\n",
      "SubGD iter. 120/499: loss=4.864240181709695, w0=71.49939999999994, w1=12.209836073165922\n",
      "SubGD iter. 121/499: loss=4.782724768945274, w0=71.66543999999993, w1=12.357476521898663\n",
      "SubGD iter. 122/499: loss=4.715208074668146, w0=71.81593999999993, w1=12.489808881940231\n",
      "SubGD iter. 123/499: loss=4.660146063774263, w0=71.95369999999993, w1=12.61241815731924\n",
      "SubGD iter. 124/499: loss=4.6138484530642305, w0=72.08067999999993, w1=12.72136694447209\n",
      "SubGD iter. 125/499: loss=4.576038370591772, w0=72.19435999999993, w1=12.815458765817555\n",
      "SubGD iter. 126/499: loss=4.5466498510490485, w0=72.29697999999993, w1=12.900592769395283\n",
      "SubGD iter. 127/499: loss=4.52256354498898, w0=72.38965999999994, w1=12.975769654279864\n",
      "SubGD iter. 128/499: loss=4.503077398110064, w0=72.47435999999993, w1=13.043963349302214\n",
      "SubGD iter. 129/499: loss=4.48701242784229, w0=72.55093999999994, w1=13.103277584601631\n",
      "SubGD iter. 130/499: loss=4.474309393425123, w0=72.62051999999994, w1=13.156032035188087\n",
      "SubGD iter. 131/499: loss=4.46416666915661, w0=72.68239999999994, w1=13.201870566969104\n",
      "SubGD iter. 132/499: loss=4.4562021249361505, w0=72.73769999999995, w1=13.241365939188638\n",
      "SubGD iter. 133/499: loss=4.449913837301212, w0=72.78823999999994, w1=13.276636749385691\n",
      "SubGD iter. 134/499: loss=4.44473944823277, w0=72.83401999999994, w1=13.30811258674963\n",
      "SubGD iter. 135/499: loss=4.440577063150136, w0=72.87447999999993, w1=13.335151520404949\n",
      "SubGD iter. 136/499: loss=4.437396761058311, w0=72.91115999999994, w1=13.358430829065401\n",
      "SubGD iter. 137/499: loss=4.434846528707501, w0=72.94447999999994, w1=13.379256780851206\n",
      "SubGD iter. 138/499: loss=4.432811551436954, w0=72.97401999999994, w1=13.396406376905487\n",
      "SubGD iter. 139/499: loss=4.431233661858323, w0=73.00103999999993, w1=13.412189120459313\n",
      "SubGD iter. 140/499: loss=4.429913776135239, w0=73.02483999999993, w1=13.425274895313027\n",
      "SubGD iter. 141/499: loss=4.428916084676416, w0=73.04625999999993, w1=13.436222021322246\n",
      "SubGD iter. 142/499: loss=4.428112214589721, w0=73.06613999999993, w1=13.447440046601196\n",
      "SubGD iter. 143/499: loss=4.427409042227935, w0=73.08405999999994, w1=13.457856669597836\n",
      "SubGD iter. 144/499: loss=4.42682578499695, w0=73.10029999999993, w1=13.467335162734816\n",
      "SubGD iter. 145/499: loss=4.42636960336517, w0=73.11401999999994, w1=13.475393628220166\n",
      "SubGD iter. 146/499: loss=4.426030178383454, w0=73.12619999999994, w1=13.482212864437836\n",
      "SubGD iter. 147/499: loss=4.425766799370375, w0=73.13697999999994, w1=13.487997256911402\n",
      "SubGD iter. 148/499: loss=4.4255559290079125, w0=73.14747999999993, w1=13.493479439140195\n",
      "SubGD iter. 149/499: loss=4.425362490640805, w0=73.15699999999993, w1=13.498468271254307\n",
      "SubGD iter. 150/499: loss=4.425203617363585, w0=73.16595999999993, w1=13.502946804297073\n",
      "SubGD iter. 151/499: loss=4.4250685279472615, w0=73.17365999999993, w1=13.506576029236195\n",
      "SubGD iter. 152/499: loss=4.424972782434856, w0=73.18009999999992, w1=13.50976866545505\n",
      "SubGD iter. 153/499: loss=4.424903121175713, w0=73.18597999999993, w1=13.512571468366527\n",
      "SubGD iter. 154/499: loss=4.424846327975847, w0=73.19143999999993, w1=13.515117410468799\n",
      "SubGD iter. 155/499: loss=4.4247992325337595, w0=73.19605999999993, w1=13.517185961787629\n",
      "SubGD iter. 156/499: loss=4.424766409404001, w0=73.19969999999994, w1=13.518069930874226\n",
      "SubGD iter. 157/499: loss=4.424747663519319, w0=73.20291999999993, w1=13.518854137766956\n",
      "SubGD iter. 158/499: loss=4.424732182668888, w0=73.20599999999993, w1=13.519529839268813\n",
      "SubGD iter. 159/499: loss=4.424717978422431, w0=73.20907999999993, w1=13.52020554077067\n",
      "SubGD iter. 160/499: loss=4.424704053977274, w0=73.21201999999992, w1=13.521000216270458\n",
      "SubGD iter. 161/499: loss=4.424690803821346, w0=73.21495999999992, w1=13.521794891770247\n",
      "SubGD iter. 162/499: loss=4.424678207991666, w0=73.21747999999992, w1=13.522407209546886\n",
      "SubGD iter. 163/499: loss=4.424668911605651, w0=73.21985999999993, w1=13.52273290846083\n",
      "SubGD iter. 164/499: loss=4.424660947850395, w0=73.22181999999992, w1=13.52299116402868\n",
      "SubGD iter. 165/499: loss=4.424655537307836, w0=73.22363999999992, w1=13.523086524955751\n",
      "SubGD iter. 166/499: loss=4.424651183353665, w0=73.22503999999992, w1=13.522757010157338\n",
      "SubGD iter. 167/499: loss=4.424648228239375, w0=73.22643999999993, w1=13.522427495358926\n",
      "SubGD iter. 168/499: loss=4.424645273125087, w0=73.22783999999993, w1=13.522097980560513\n",
      "SubGD iter. 169/499: loss=4.424642498283796, w0=73.22895999999993, w1=13.522225963141185\n",
      "SubGD iter. 170/499: loss=4.424640682884451, w0=73.23007999999993, w1=13.522353945721857\n",
      "SubGD iter. 171/499: loss=4.424638867485107, w0=73.23119999999993, w1=13.522481928302529\n",
      "SubGD iter. 172/499: loss=4.424637300852185, w0=73.23189999999992, w1=13.523013495746964\n",
      "SubGD iter. 173/499: loss=4.424636258655272, w0=73.23273999999992, w1=13.523266279565734\n",
      "SubGD iter. 174/499: loss=4.424635339412461, w0=73.23329999999991, w1=13.52374123507471\n",
      "SubGD iter. 175/499: loss=4.424634640448436, w0=73.23399999999991, w1=13.52393740695802\n",
      "SubGD iter. 176/499: loss=4.424633962890472, w0=73.2345599999999, w1=13.523888604707635\n",
      "SubGD iter. 177/499: loss=4.424633633499305, w0=73.23497999999991, w1=13.524118586082915\n",
      "SubGD iter. 178/499: loss=4.424633305940114, w0=73.23539999999991, w1=13.524348567458196\n",
      "SubGD iter. 179/499: loss=4.424632978380924, w0=73.23581999999992, w1=13.524578548833476\n",
      "SubGD iter. 180/499: loss=4.424632650821734, w0=73.23623999999992, w1=13.524808530208757\n",
      "SubGD iter. 181/499: loss=4.424632389924094, w0=73.23651999999993, w1=13.524964267899207\n",
      "SubGD iter. 182/499: loss=4.4246322432751946, w0=73.23679999999993, w1=13.525120005589658\n",
      "SubGD iter. 183/499: loss=4.424632096626297, w0=73.23707999999993, w1=13.525275743280108\n",
      "SubGD iter. 184/499: loss=4.424631993328701, w0=73.23721999999994, w1=13.52517442419614\n",
      "SubGD iter. 185/499: loss=4.424631950663619, w0=73.23735999999994, w1=13.52507310511217\n",
      "SubGD iter. 186/499: loss=4.424631907998538, w0=73.23749999999994, w1=13.524971786028201\n",
      "SubGD iter. 187/499: loss=4.424631865333457, w0=73.23763999999994, w1=13.524870466944233\n",
      "SubGD iter. 188/499: loss=4.424631822668376, w0=73.23777999999994, w1=13.524769147860264\n",
      "SubGD iter. 189/499: loss=4.424631782685973, w0=73.23805999999995, w1=13.524924885550714\n",
      "SubGD iter. 190/499: loss=4.424631746545009, w0=73.23819999999995, w1=13.524823566466745\n",
      "SubGD iter. 191/499: loss=4.424631703879928, w0=73.23833999999995, w1=13.524722247382776\n",
      "SubGD iter. 192/499: loss=4.424631661214847, w0=73.23847999999995, w1=13.524620928298807\n",
      "SubGD iter. 193/499: loss=4.424631622214843, w0=73.23847999999995, w1=13.524586781150251\n",
      "SubGD iter. 194/499: loss=4.424631620549089, w0=73.23847999999995, w1=13.524552634001695\n",
      "SubGD iter. 195/499: loss=4.4246316188833354, w0=73.23847999999995, w1=13.52451848685314\n",
      "SubGD iter. 196/499: loss=4.424631617217582, w0=73.23847999999995, w1=13.524484339704584\n",
      "SubGD iter. 197/499: loss=4.424631615551827, w0=73.23847999999995, w1=13.524450192556028\n",
      "SubGD iter. 198/499: loss=4.424631613886074, w0=73.23847999999995, w1=13.524416045407472\n",
      "SubGD iter. 199/499: loss=4.4246316122203195, w0=73.23847999999995, w1=13.524381898258916\n",
      "SubGD iter. 200/499: loss=4.424631615444577, w0=73.23861999999995, w1=13.52460480788478\n",
      "SubGD iter. 201/499: loss=4.424631621428463, w0=73.23861999999995, w1=13.524570660736224\n",
      "SubGD iter. 202/499: loss=4.424631619762709, w0=73.23861999999995, w1=13.524536513587668\n",
      "SubGD iter. 203/499: loss=4.424631618096955, w0=73.23861999999995, w1=13.524502366439112\n",
      "SubGD iter. 204/499: loss=4.424631616431202, w0=73.23861999999995, w1=13.524468219290556\n",
      "SubGD iter. 205/499: loss=4.424631614765447, w0=73.23861999999995, w1=13.524434072142\n",
      "SubGD iter. 206/499: loss=4.4246316130996926, w0=73.23861999999995, w1=13.524399924993444\n",
      "SubGD iter. 207/499: loss=4.424631611433939, w0=73.23861999999995, w1=13.524365777844888\n",
      "SubGD iter. 208/499: loss=4.424631609768185, w0=73.23861999999995, w1=13.524331630696333\n",
      "SubGD iter. 209/499: loss=4.42463160810243, w0=73.23861999999995, w1=13.524297483547777\n",
      "SubGD iter. 210/499: loss=4.4246316143257935, w0=73.23875999999996, w1=13.52452039317364\n",
      "SubGD iter. 211/499: loss=4.424631617310576, w0=73.23875999999996, w1=13.524486246025084\n",
      "SubGD iter. 212/499: loss=4.424631615644821, w0=73.23875999999996, w1=13.524452098876528\n",
      "SubGD iter. 213/499: loss=4.4246316139790665, w0=73.23875999999996, w1=13.524417951727973\n",
      "SubGD iter. 214/499: loss=4.424631612313314, w0=73.23875999999996, w1=13.524383804579417\n",
      "SubGD iter. 215/499: loss=4.424631610647558, w0=73.23875999999996, w1=13.52434965743086\n",
      "SubGD iter. 216/499: loss=4.424631608981804, w0=73.23875999999996, w1=13.524315510282305\n",
      "SubGD iter. 217/499: loss=4.424631607316051, w0=73.23875999999996, w1=13.524281363133749\n",
      "SubGD iter. 218/499: loss=4.424631605650297, w0=73.23875999999996, w1=13.524247215985193\n",
      "SubGD iter. 219/499: loss=4.424631603984544, w0=73.23875999999996, w1=13.524213068836637\n",
      "SubGD iter. 220/499: loss=4.424631613207011, w0=73.23889999999996, w1=13.5244359784625\n",
      "SubGD iter. 221/499: loss=4.424631613192687, w0=73.23889999999996, w1=13.524401831313945\n",
      "SubGD iter. 222/499: loss=4.424631611526933, w0=73.23889999999996, w1=13.524367684165389\n",
      "SubGD iter. 223/499: loss=4.424631609861179, w0=73.23889999999996, w1=13.524333537016833\n",
      "SubGD iter. 224/499: loss=4.4246316081954244, w0=73.23889999999996, w1=13.524299389868277\n",
      "SubGD iter. 225/499: loss=4.424631606529672, w0=73.23889999999996, w1=13.524265242719721\n",
      "SubGD iter. 226/499: loss=4.424631604863917, w0=73.23889999999996, w1=13.524231095571166\n",
      "SubGD iter. 227/499: loss=4.424631603198163, w0=73.23889999999996, w1=13.52419694842261\n",
      "SubGD iter. 228/499: loss=4.424631601532409, w0=73.23889999999996, w1=13.524162801274054\n",
      "SubGD iter. 229/499: loss=4.4246316012143305, w0=73.23903999999996, w1=13.524385710899917\n",
      "SubGD iter. 230/499: loss=4.424631610740552, w0=73.23903999999996, w1=13.524351563751361\n",
      "SubGD iter. 231/499: loss=4.424631609074798, w0=73.23903999999996, w1=13.524317416602806\n",
      "SubGD iter. 232/499: loss=4.424631607409045, w0=73.23903999999996, w1=13.52428326945425\n",
      "SubGD iter. 233/499: loss=4.424631605743291, w0=73.23903999999996, w1=13.524249122305694\n",
      "SubGD iter. 234/499: loss=4.424631604077536, w0=73.23903999999996, w1=13.524214975157138\n",
      "SubGD iter. 235/499: loss=4.424631602411782, w0=73.23903999999996, w1=13.524180828008582\n",
      "SubGD iter. 236/499: loss=4.424631600746029, w0=73.23903999999996, w1=13.524146680860026\n",
      "SubGD iter. 237/499: loss=4.424631599080274, w0=73.23903999999996, w1=13.52411253371147\n",
      "SubGD iter. 238/499: loss=4.424631597414521, w0=73.23903999999996, w1=13.524078386562914\n",
      "SubGD iter. 239/499: loss=4.424631600095546, w0=73.23917999999996, w1=13.524301296188778\n",
      "SubGD iter. 240/499: loss=4.424631606622665, w0=73.23917999999996, w1=13.524267149040222\n",
      "SubGD iter. 241/499: loss=4.42463160495691, w0=73.23917999999996, w1=13.524233001891666\n",
      "SubGD iter. 242/499: loss=4.424631603291156, w0=73.23917999999996, w1=13.52419885474311\n",
      "SubGD iter. 243/499: loss=4.424631601625403, w0=73.23917999999996, w1=13.524164707594554\n",
      "SubGD iter. 244/499: loss=4.424631599959648, w0=73.23917999999996, w1=13.524130560445998\n",
      "SubGD iter. 245/499: loss=4.424631598293895, w0=73.23917999999996, w1=13.524096413297443\n",
      "SubGD iter. 246/499: loss=4.42463159662814, w0=73.23917999999996, w1=13.524062266148887\n",
      "SubGD iter. 247/499: loss=4.424631594962387, w0=73.23917999999996, w1=13.52402811900033\n",
      "SubGD iter. 248/499: loss=4.424631593296633, w0=73.23917999999996, w1=13.523993971851775\n",
      "SubGD iter. 249/499: loss=4.424631598976763, w0=73.23931999999996, w1=13.524216881477638\n",
      "SubGD iter. 250/499: loss=4.424631602504776, w0=73.23931999999996, w1=13.524182734329083\n",
      "SubGD iter. 251/499: loss=4.424631600839022, w0=73.23931999999996, w1=13.524148587180527\n",
      "SubGD iter. 252/499: loss=4.424631599173268, w0=73.23931999999996, w1=13.52411444003197\n",
      "SubGD iter. 253/499: loss=4.4246315975075134, w0=73.23931999999996, w1=13.524080292883415\n",
      "SubGD iter. 254/499: loss=4.424631595841761, w0=73.23931999999996, w1=13.524046145734859\n",
      "SubGD iter. 255/499: loss=4.424631594176006, w0=73.23931999999996, w1=13.524011998586303\n",
      "SubGD iter. 256/499: loss=4.424631592510253, w0=73.23931999999996, w1=13.523977851437747\n",
      "SubGD iter. 257/499: loss=4.424631590844498, w0=73.23931999999996, w1=13.523943704289191\n",
      "SubGD iter. 258/499: loss=4.424631589178745, w0=73.23931999999996, w1=13.523909557140636\n",
      "SubGD iter. 259/499: loss=4.42463159785798, w0=73.23945999999997, w1=13.524132466766499\n",
      "SubGD iter. 260/499: loss=4.424631598386888, w0=73.23945999999997, w1=13.524098319617943\n",
      "SubGD iter. 261/499: loss=4.424631596721134, w0=73.23945999999997, w1=13.524064172469387\n",
      "SubGD iter. 262/499: loss=4.42463159505538, w0=73.23945999999997, w1=13.524030025320831\n",
      "SubGD iter. 263/499: loss=4.424631593389626, w0=73.23945999999997, w1=13.523995878172276\n",
      "SubGD iter. 264/499: loss=4.424631591723872, w0=73.23945999999997, w1=13.52396173102372\n",
      "SubGD iter. 265/499: loss=4.424631590058119, w0=73.23945999999997, w1=13.523927583875164\n",
      "SubGD iter. 266/499: loss=4.424631588392364, w0=73.23945999999997, w1=13.523893436726608\n",
      "SubGD iter. 267/499: loss=4.424631586726611, w0=73.23945999999997, w1=13.523859289578052\n",
      "SubGD iter. 268/499: loss=4.424631585865299, w0=73.23959999999997, w1=13.524082199203916\n",
      "SubGD iter. 269/499: loss=4.424631595934754, w0=73.23959999999997, w1=13.52404805205536\n",
      "SubGD iter. 270/499: loss=4.424631594269, w0=73.23959999999997, w1=13.524013904906804\n",
      "SubGD iter. 271/499: loss=4.424631592603246, w0=73.23959999999997, w1=13.523979757758248\n",
      "SubGD iter. 272/499: loss=4.424631590937492, w0=73.23959999999997, w1=13.523945610609692\n",
      "SubGD iter. 273/499: loss=4.424631589271739, w0=73.23959999999997, w1=13.523911463461136\n",
      "SubGD iter. 274/499: loss=4.424631587605984, w0=73.23959999999997, w1=13.52387731631258\n",
      "SubGD iter. 275/499: loss=4.424631585940229, w0=73.23959999999997, w1=13.523843169164024\n",
      "SubGD iter. 276/499: loss=4.424631584274477, w0=73.23959999999997, w1=13.523809022015469\n",
      "SubGD iter. 277/499: loss=4.424631582608722, w0=73.23959999999997, w1=13.523774874866913\n",
      "SubGD iter. 278/499: loss=4.424631584746516, w0=73.23973999999997, w1=13.523997784492776\n",
      "SubGD iter. 279/499: loss=4.424631591816865, w0=73.23973999999997, w1=13.52396363734422\n",
      "SubGD iter. 280/499: loss=4.424631590151112, w0=73.23973999999997, w1=13.523929490195664\n",
      "SubGD iter. 281/499: loss=4.424631588485357, w0=73.23973999999997, w1=13.523895343047108\n",
      "SubGD iter. 282/499: loss=4.424631586819603, w0=73.23973999999997, w1=13.523861195898553\n",
      "SubGD iter. 283/499: loss=4.4246315851538505, w0=73.23973999999997, w1=13.523827048749997\n",
      "SubGD iter. 284/499: loss=4.424631583488097, w0=73.23973999999997, w1=13.52379290160144\n",
      "SubGD iter. 285/499: loss=4.424631581822342, w0=73.23973999999997, w1=13.523758754452885\n",
      "SubGD iter. 286/499: loss=4.424631580156588, w0=73.23973999999997, w1=13.52372460730433\n",
      "SubGD iter. 287/499: loss=4.4246315784908345, w0=73.23973999999997, w1=13.523690460155773\n",
      "SubGD iter. 288/499: loss=4.424631583627733, w0=73.23987999999997, w1=13.523913369781637\n",
      "SubGD iter. 289/499: loss=4.424631587698977, w0=73.23987999999997, w1=13.52387922263308\n",
      "SubGD iter. 290/499: loss=4.4246315860332235, w0=73.23987999999997, w1=13.523845075484525\n",
      "SubGD iter. 291/499: loss=4.42463158436747, w0=73.23987999999997, w1=13.523810928335969\n",
      "SubGD iter. 292/499: loss=4.424631582701715, w0=73.23987999999997, w1=13.523776781187413\n",
      "SubGD iter. 293/499: loss=4.424631581035962, w0=73.23987999999997, w1=13.523742634038857\n",
      "SubGD iter. 294/499: loss=4.4246315793702085, w0=73.23987999999997, w1=13.523708486890301\n",
      "SubGD iter. 295/499: loss=4.424631577704454, w0=73.23987999999997, w1=13.523674339741746\n",
      "SubGD iter. 296/499: loss=4.4246315760387, w0=73.23987999999997, w1=13.52364019259319\n",
      "SubGD iter. 297/499: loss=4.424631574372945, w0=73.23987999999997, w1=13.523606045444634\n",
      "SubGD iter. 298/499: loss=4.424631582508949, w0=73.24001999999997, w1=13.523828955070497\n",
      "SubGD iter. 299/499: loss=4.424631583581089, w0=73.24001999999997, w1=13.523794807921941\n",
      "SubGD iter. 300/499: loss=4.424631581915335, w0=73.24001999999997, w1=13.523760660773386\n",
      "SubGD iter. 301/499: loss=4.424631580249582, w0=73.24001999999997, w1=13.52372651362483\n",
      "SubGD iter. 302/499: loss=4.424631578583828, w0=73.24001999999997, w1=13.523692366476274\n",
      "SubGD iter. 303/499: loss=4.424631576918073, w0=73.24001999999997, w1=13.523658219327718\n",
      "SubGD iter. 304/499: loss=4.42463157525232, w0=73.24001999999997, w1=13.523624072179162\n",
      "SubGD iter. 305/499: loss=4.424631573586566, w0=73.24001999999997, w1=13.523589925030606\n",
      "SubGD iter. 306/499: loss=4.424631571920812, w0=73.24001999999997, w1=13.52355577788205\n",
      "SubGD iter. 307/499: loss=4.4246315705162695, w0=73.24015999999997, w1=13.523778687507914\n",
      "SubGD iter. 308/499: loss=4.424631581128955, w0=73.24015999999997, w1=13.523744540359358\n",
      "SubGD iter. 309/499: loss=4.424631579463202, w0=73.24015999999997, w1=13.523710393210802\n",
      "SubGD iter. 310/499: loss=4.424631577797447, w0=73.24015999999997, w1=13.523676246062246\n",
      "SubGD iter. 311/499: loss=4.424631576131694, w0=73.24015999999997, w1=13.52364209891369\n",
      "SubGD iter. 312/499: loss=4.4246315744659395, w0=73.24015999999997, w1=13.523607951765134\n",
      "SubGD iter. 313/499: loss=4.424631572800186, w0=73.24015999999997, w1=13.523573804616579\n",
      "SubGD iter. 314/499: loss=4.424631571134432, w0=73.24015999999997, w1=13.523539657468023\n",
      "SubGD iter. 315/499: loss=4.424631569468678, w0=73.24015999999997, w1=13.523505510319467\n",
      "SubGD iter. 316/499: loss=4.4246315678029235, w0=73.24015999999997, w1=13.523471363170911\n",
      "SubGD iter. 317/499: loss=4.424631569397486, w0=73.24029999999998, w1=13.523694272796774\n",
      "SubGD iter. 318/499: loss=4.424631577011067, w0=73.24029999999998, w1=13.523660125648219\n",
      "SubGD iter. 319/499: loss=4.424631575345313, w0=73.24029999999998, w1=13.523625978499663\n",
      "SubGD iter. 320/499: loss=4.4246315736795605, w0=73.24029999999998, w1=13.523591831351107\n",
      "SubGD iter. 321/499: loss=4.424631572013806, w0=73.24029999999998, w1=13.52355768420255\n",
      "SubGD iter. 322/499: loss=4.424631570348051, w0=73.24029999999998, w1=13.523523537053995\n",
      "SubGD iter. 323/499: loss=4.4246315686822975, w0=73.24029999999998, w1=13.52348938990544\n",
      "SubGD iter. 324/499: loss=4.424631567016543, w0=73.24029999999998, w1=13.523455242756883\n",
      "SubGD iter. 325/499: loss=4.42463156535079, w0=73.24029999999998, w1=13.523421095608327\n",
      "SubGD iter. 326/499: loss=4.424631563685035, w0=73.24029999999998, w1=13.523386948459772\n",
      "SubGD iter. 327/499: loss=4.424631568278702, w0=73.24043999999998, w1=13.523609858085635\n",
      "SubGD iter. 328/499: loss=4.424631572893179, w0=73.24043999999998, w1=13.523575710937079\n",
      "SubGD iter. 329/499: loss=4.424631571227425, w0=73.24043999999998, w1=13.523541563788523\n",
      "SubGD iter. 330/499: loss=4.424631569561671, w0=73.24043999999998, w1=13.523507416639967\n",
      "SubGD iter. 331/499: loss=4.424631567895917, w0=73.24043999999998, w1=13.523473269491411\n",
      "SubGD iter. 332/499: loss=4.424631566230163, w0=73.24043999999998, w1=13.523439122342856\n",
      "SubGD iter. 333/499: loss=4.424631564564409, w0=73.24043999999998, w1=13.5234049751943\n",
      "SubGD iter. 334/499: loss=4.4246315628986554, w0=73.24043999999998, w1=13.523370828045744\n",
      "SubGD iter. 335/499: loss=4.424631561232902, w0=73.24043999999998, w1=13.523336680897188\n",
      "SubGD iter. 336/499: loss=4.424631559567148, w0=73.24043999999998, w1=13.523302533748632\n",
      "SubGD iter. 337/499: loss=4.42463156715992, w0=73.24057999999998, w1=13.523525443374496\n",
      "SubGD iter. 338/499: loss=4.4246315687752915, w0=73.24057999999998, w1=13.52349129622594\n",
      "SubGD iter. 339/499: loss=4.424631567109538, w0=73.24057999999998, w1=13.523457149077384\n",
      "SubGD iter. 340/499: loss=4.424631565443783, w0=73.24057999999998, w1=13.523423001928828\n",
      "SubGD iter. 341/499: loss=4.424631563778029, w0=73.24057999999998, w1=13.523388854780272\n",
      "SubGD iter. 342/499: loss=4.424631562112276, w0=73.24057999999998, w1=13.523354707631716\n",
      "SubGD iter. 343/499: loss=4.424631560446521, w0=73.24057999999998, w1=13.52332056048316\n",
      "SubGD iter. 344/499: loss=4.424631558780768, w0=73.24057999999998, w1=13.523286413334604\n",
      "SubGD iter. 345/499: loss=4.424631557115013, w0=73.24057999999998, w1=13.523252266186049\n",
      "SubGD iter. 346/499: loss=4.424631555449259, w0=73.24057999999998, w1=13.523218119037493\n",
      "SubGD iter. 347/499: loss=4.4246315660411355, w0=73.24071999999998, w1=13.523441028663356\n",
      "SubGD iter. 348/499: loss=4.424631564657403, w0=73.24071999999998, w1=13.5234068815148\n",
      "SubGD iter. 349/499: loss=4.4246315629916495, w0=73.24071999999998, w1=13.523372734366244\n",
      "SubGD iter. 350/499: loss=4.424631561325894, w0=73.24071999999998, w1=13.523338587217689\n",
      "SubGD iter. 351/499: loss=4.424631559660142, w0=73.24071999999998, w1=13.523304440069133\n",
      "SubGD iter. 352/499: loss=4.4246315579943865, w0=73.24071999999998, w1=13.523270292920577\n",
      "SubGD iter. 353/499: loss=4.424631556328634, w0=73.24071999999998, w1=13.523236145772021\n",
      "SubGD iter. 354/499: loss=4.424631554662879, w0=73.24071999999998, w1=13.523201998623465\n",
      "SubGD iter. 355/499: loss=4.424631552997125, w0=73.24071999999998, w1=13.52316785147491\n",
      "SubGD iter. 356/499: loss=4.424631554048455, w0=73.24085999999998, w1=13.523390761100773\n",
      "SubGD iter. 357/499: loss=4.424631562205269, w0=73.24085999999998, w1=13.523356613952217\n",
      "SubGD iter. 358/499: loss=4.424631560539515, w0=73.24085999999998, w1=13.523322466803661\n",
      "SubGD iter. 359/499: loss=4.42463155887376, w0=73.24085999999998, w1=13.523288319655105\n",
      "SubGD iter. 360/499: loss=4.424631557208007, w0=73.24085999999998, w1=13.52325417250655\n",
      "SubGD iter. 361/499: loss=4.424631555542253, w0=73.24085999999998, w1=13.523220025357993\n",
      "SubGD iter. 362/499: loss=4.424631553876498, w0=73.24085999999998, w1=13.523185878209437\n",
      "SubGD iter. 363/499: loss=4.424631552210745, w0=73.24085999999998, w1=13.523151731060882\n",
      "SubGD iter. 364/499: loss=4.424631550544991, w0=73.24085999999998, w1=13.523117583912326\n",
      "SubGD iter. 365/499: loss=4.424631548879238, w0=73.24085999999998, w1=13.52308343676377\n",
      "SubGD iter. 366/499: loss=4.4246315529296725, w0=73.24099999999999, w1=13.523306346389633\n",
      "SubGD iter. 367/499: loss=4.4246315580873805, w0=73.24099999999999, w1=13.523272199241077\n",
      "SubGD iter. 368/499: loss=4.424631556421626, w0=73.24099999999999, w1=13.523238052092521\n",
      "SubGD iter. 369/499: loss=4.424631554755873, w0=73.24099999999999, w1=13.523203904943966\n",
      "SubGD iter. 370/499: loss=4.424631553090119, w0=73.24099999999999, w1=13.52316975779541\n",
      "SubGD iter. 371/499: loss=4.424631551424365, w0=73.24099999999999, w1=13.523135610646854\n",
      "SubGD iter. 372/499: loss=4.42463154975861, w0=73.24099999999999, w1=13.523101463498298\n",
      "SubGD iter. 373/499: loss=4.424631548092857, w0=73.24099999999999, w1=13.523067316349742\n",
      "SubGD iter. 374/499: loss=4.424631546427102, w0=73.24099999999999, w1=13.523033169201186\n",
      "SubGD iter. 375/499: loss=4.42463154476135, w0=73.24099999999999, w1=13.52299902205263\n",
      "SubGD iter. 376/499: loss=4.424631551810888, w0=73.24113999999999, w1=13.523221931678494\n",
      "SubGD iter. 377/499: loss=4.424631553969492, w0=73.24113999999999, w1=13.523187784529938\n",
      "SubGD iter. 378/499: loss=4.4246315523037385, w0=73.24113999999999, w1=13.523153637381382\n",
      "SubGD iter. 379/499: loss=4.424631550637984, w0=73.24113999999999, w1=13.523119490232826\n",
      "SubGD iter. 380/499: loss=4.424631548972231, w0=73.24113999999999, w1=13.52308534308427\n",
      "SubGD iter. 381/499: loss=4.4246315473064755, w0=73.24113999999999, w1=13.523051195935714\n",
      "SubGD iter. 382/499: loss=4.4246315456407235, w0=73.24113999999999, w1=13.523017048787159\n",
      "SubGD iter. 383/499: loss=4.424631543974968, w0=73.24113999999999, w1=13.522982901638603\n",
      "SubGD iter. 384/499: loss=4.424631542309215, w0=73.24113999999999, w1=13.522948754490047\n",
      "SubGD iter. 385/499: loss=4.424631540643461, w0=73.24113999999999, w1=13.522914607341491\n",
      "SubGD iter. 386/499: loss=4.424631550692105, w0=73.24127999999999, w1=13.523137516967354\n",
      "SubGD iter. 387/499: loss=4.424631549851604, w0=73.24127999999999, w1=13.523103369818799\n",
      "SubGD iter. 388/499: loss=4.424631548185851, w0=73.24127999999999, w1=13.523069222670243\n",
      "SubGD iter. 389/499: loss=4.4246315465200965, w0=73.24127999999999, w1=13.523035075521687\n",
      "SubGD iter. 390/499: loss=4.424631544854342, w0=73.24127999999999, w1=13.523000928373131\n",
      "SubGD iter. 391/499: loss=4.424631543188589, w0=73.24127999999999, w1=13.522966781224575\n",
      "SubGD iter. 392/499: loss=4.424631541522835, w0=73.24127999999999, w1=13.52293263407602\n",
      "SubGD iter. 393/499: loss=4.424631539857081, w0=73.24127999999999, w1=13.522898486927463\n",
      "SubGD iter. 394/499: loss=4.424631538191327, w0=73.24127999999999, w1=13.522864339778907\n",
      "SubGD iter. 395/499: loss=4.424631538699424, w0=73.24141999999999, w1=13.523087249404771\n",
      "SubGD iter. 396/499: loss=4.4246315473994695, w0=73.24141999999999, w1=13.523053102256215\n",
      "SubGD iter. 397/499: loss=4.424631545733717, w0=73.24141999999999, w1=13.52301895510766\n",
      "SubGD iter. 398/499: loss=4.424631544067962, w0=73.24141999999999, w1=13.522984807959103\n",
      "SubGD iter. 399/499: loss=4.424631542402208, w0=73.24141999999999, w1=13.522950660810547\n",
      "SubGD iter. 400/499: loss=4.4246315407364545, w0=73.24141999999999, w1=13.522916513661992\n",
      "SubGD iter. 401/499: loss=4.424631539070701, w0=73.24141999999999, w1=13.522882366513436\n",
      "SubGD iter. 402/499: loss=4.424631537404946, w0=73.24141999999999, w1=13.52284821936488\n",
      "SubGD iter. 403/499: loss=4.424631535739192, w0=73.24141999999999, w1=13.522814072216324\n",
      "SubGD iter. 404/499: loss=4.424631534073439, w0=73.24141999999999, w1=13.522779925067768\n",
      "SubGD iter. 405/499: loss=4.424631537580641, w0=73.24155999999999, w1=13.523002834693632\n",
      "SubGD iter. 406/499: loss=4.424631543281581, w0=73.24155999999999, w1=13.522968687545076\n",
      "SubGD iter. 407/499: loss=4.424631541615828, w0=73.24155999999999, w1=13.52293454039652\n",
      "SubGD iter. 408/499: loss=4.424631539950075, w0=73.24155999999999, w1=13.522900393247964\n",
      "SubGD iter. 409/499: loss=4.42463153828432, w0=73.24155999999999, w1=13.522866246099408\n",
      "SubGD iter. 410/499: loss=4.424631536618566, w0=73.24155999999999, w1=13.522832098950852\n",
      "SubGD iter. 411/499: loss=4.4246315349528125, w0=73.24155999999999, w1=13.522797951802296\n",
      "SubGD iter. 412/499: loss=4.424631533287058, w0=73.24155999999999, w1=13.52276380465374\n",
      "SubGD iter. 413/499: loss=4.424631531621305, w0=73.24155999999999, w1=13.522729657505185\n",
      "SubGD iter. 414/499: loss=4.42463152995555, w0=73.24155999999999, w1=13.522695510356629\n",
      "SubGD iter. 415/499: loss=4.424631536461858, w0=73.2417, w1=13.522918419982492\n",
      "SubGD iter. 416/499: loss=4.424631540335804, w0=73.24155999999999, w1=13.522905630681025\n",
      "SubGD iter. 417/499: loss=4.424631538539812, w0=73.24155999999999, w1=13.52287148353247\n",
      "SubGD iter. 418/499: loss=4.424631536874056, w0=73.24155999999999, w1=13.522837336383914\n",
      "SubGD iter. 419/499: loss=4.424631535208302, w0=73.24155999999999, w1=13.522803189235358\n",
      "SubGD iter. 420/499: loss=4.424631533542549, w0=73.24155999999999, w1=13.522769042086802\n",
      "SubGD iter. 421/499: loss=4.424631531876795, w0=73.24155999999999, w1=13.522734894938246\n",
      "SubGD iter. 422/499: loss=4.424631530211041, w0=73.24155999999999, w1=13.52270074778969\n",
      "SubGD iter. 423/499: loss=4.424631534794037, w0=73.2417, w1=13.522923657415554\n",
      "SubGD iter. 424/499: loss=4.424631540431494, w0=73.24155999999999, w1=13.522910868114087\n",
      "SubGD iter. 425/499: loss=4.4246315387953015, w0=73.24155999999999, w1=13.522876720965531\n",
      "SubGD iter. 426/499: loss=4.424631537129548, w0=73.24155999999999, w1=13.522842573816975\n",
      "SubGD iter. 427/499: loss=4.424631535463794, w0=73.24155999999999, w1=13.52280842666842\n",
      "SubGD iter. 428/499: loss=4.424631533798039, w0=73.24155999999999, w1=13.522774279519863\n",
      "SubGD iter. 429/499: loss=4.4246315321322855, w0=73.24155999999999, w1=13.522740132371307\n",
      "SubGD iter. 430/499: loss=4.424631530466532, w0=73.24155999999999, w1=13.522705985222752\n",
      "SubGD iter. 431/499: loss=4.424631533126218, w0=73.2417, w1=13.522928894848615\n",
      "SubGD iter. 432/499: loss=4.424631540527184, w0=73.24155999999999, w1=13.522916105547148\n",
      "SubGD iter. 433/499: loss=4.424631539050792, w0=73.24155999999999, w1=13.522881958398592\n",
      "SubGD iter. 434/499: loss=4.4246315373850384, w0=73.24155999999999, w1=13.522847811250037\n",
      "SubGD iter. 435/499: loss=4.424631535719284, w0=73.24155999999999, w1=13.52281366410148\n",
      "SubGD iter. 436/499: loss=4.42463153405353, w0=73.24155999999999, w1=13.522779516952925\n",
      "SubGD iter. 437/499: loss=4.424631532387776, w0=73.24155999999999, w1=13.522745369804369\n",
      "SubGD iter. 438/499: loss=4.424631530722022, w0=73.24155999999999, w1=13.522711222655813\n",
      "SubGD iter. 439/499: loss=4.424631531458397, w0=73.2417, w1=13.522934132281677\n",
      "SubGD iter. 440/499: loss=4.424631540622873, w0=73.24155999999999, w1=13.52292134298021\n",
      "SubGD iter. 441/499: loss=4.424631539306283, w0=73.24155999999999, w1=13.522887195831654\n",
      "SubGD iter. 442/499: loss=4.424631537640528, w0=73.24155999999999, w1=13.522853048683098\n",
      "SubGD iter. 443/499: loss=4.4246315359747745, w0=73.24155999999999, w1=13.522818901534542\n",
      "SubGD iter. 444/499: loss=4.42463153430902, w0=73.24155999999999, w1=13.522784754385986\n",
      "SubGD iter. 445/499: loss=4.424631532643267, w0=73.24155999999999, w1=13.52275060723743\n",
      "SubGD iter. 446/499: loss=4.424631530977512, w0=73.24155999999999, w1=13.522716460088875\n",
      "SubGD iter. 447/499: loss=4.4246315297905765, w0=73.2417, w1=13.522939369714738\n",
      "SubGD iter. 448/499: loss=4.424631540718564, w0=73.24155999999999, w1=13.522926580413271\n",
      "SubGD iter. 449/499: loss=4.424631539561774, w0=73.24155999999999, w1=13.522892433264715\n",
      "SubGD iter. 450/499: loss=4.424631537896018, w0=73.24155999999999, w1=13.52285828611616\n",
      "SubGD iter. 451/499: loss=4.424631536230266, w0=73.24155999999999, w1=13.522824138967604\n",
      "SubGD iter. 452/499: loss=4.4246315345645115, w0=73.24155999999999, w1=13.522789991819048\n",
      "SubGD iter. 453/499: loss=4.424631532898756, w0=73.24155999999999, w1=13.522755844670492\n",
      "SubGD iter. 454/499: loss=4.424631531233004, w0=73.24155999999999, w1=13.522721697521936\n",
      "SubGD iter. 455/499: loss=4.424631529567249, w0=73.24155999999999, w1=13.52268755037338\n",
      "SubGD iter. 456/499: loss=4.424631538996653, w0=73.2417, w1=13.522910459999244\n",
      "SubGD iter. 457/499: loss=4.424631540190371, w0=73.24155999999999, w1=13.522897670697777\n",
      "SubGD iter. 458/499: loss=4.42463153815151, w0=73.24155999999999, w1=13.522863523549221\n",
      "SubGD iter. 459/499: loss=4.424631536485756, w0=73.24155999999999, w1=13.522829376400665\n",
      "SubGD iter. 460/499: loss=4.424631534820002, w0=73.24155999999999, w1=13.52279522925211\n",
      "SubGD iter. 461/499: loss=4.4246315331542485, w0=73.24155999999999, w1=13.522761082103553\n",
      "SubGD iter. 462/499: loss=4.424631531488495, w0=73.24155999999999, w1=13.522726934954997\n",
      "SubGD iter. 463/499: loss=4.424631529822739, w0=73.24155999999999, w1=13.522692787806442\n",
      "SubGD iter. 464/499: loss=4.424631537328833, w0=73.2417, w1=13.522915697432305\n",
      "SubGD iter. 465/499: loss=4.424631540286062, w0=73.24155999999999, w1=13.522902908130838\n",
      "SubGD iter. 466/499: loss=4.4246315384070005, w0=73.24155999999999, w1=13.522868760982282\n",
      "SubGD iter. 467/499: loss=4.424631536741246, w0=73.24155999999999, w1=13.522834613833727\n",
      "SubGD iter. 468/499: loss=4.424631535075492, w0=73.24155999999999, w1=13.52280046668517\n",
      "SubGD iter. 469/499: loss=4.424631533409738, w0=73.24155999999999, w1=13.522766319536615\n",
      "SubGD iter. 470/499: loss=4.424631531743984, w0=73.24155999999999, w1=13.522732172388059\n",
      "SubGD iter. 471/499: loss=4.424631530078232, w0=73.24155999999999, w1=13.522698025239503\n",
      "SubGD iter. 472/499: loss=4.424631535661012, w0=73.2417, w1=13.522920934865367\n",
      "SubGD iter. 473/499: loss=4.4246315403817515, w0=73.24155999999999, w1=13.5229081455639\n",
      "SubGD iter. 474/499: loss=4.424631538662491, w0=73.24155999999999, w1=13.522873998415344\n",
      "SubGD iter. 475/499: loss=4.4246315369967375, w0=73.24155999999999, w1=13.522839851266788\n",
      "SubGD iter. 476/499: loss=4.424631535330983, w0=73.24155999999999, w1=13.522805704118232\n",
      "SubGD iter. 477/499: loss=4.424631533665229, w0=73.24155999999999, w1=13.522771556969676\n",
      "SubGD iter. 478/499: loss=4.424631531999475, w0=73.24155999999999, w1=13.52273740982112\n",
      "SubGD iter. 479/499: loss=4.424631530333721, w0=73.24155999999999, w1=13.522703262672565\n",
      "SubGD iter. 480/499: loss=4.424631533993192, w0=73.2417, w1=13.522926172298428\n",
      "SubGD iter. 481/499: loss=4.424631540477442, w0=73.24155999999999, w1=13.522913382996961\n",
      "SubGD iter. 482/499: loss=4.424631538917982, w0=73.24155999999999, w1=13.522879235848405\n",
      "SubGD iter. 483/499: loss=4.424631537252228, w0=73.24155999999999, w1=13.52284508869985\n",
      "SubGD iter. 484/499: loss=4.424631535586474, w0=73.24155999999999, w1=13.522810941551294\n",
      "SubGD iter. 485/499: loss=4.42463153392072, w0=73.24155999999999, w1=13.522776794402738\n",
      "SubGD iter. 486/499: loss=4.424631532254966, w0=73.24155999999999, w1=13.522742647254182\n",
      "SubGD iter. 487/499: loss=4.424631530589211, w0=73.24155999999999, w1=13.522708500105626\n",
      "SubGD iter. 488/499: loss=4.424631532325371, w0=73.2417, w1=13.52293140973149\n",
      "SubGD iter. 489/499: loss=4.424631540573132, w0=73.24155999999999, w1=13.522918620430023\n",
      "SubGD iter. 490/499: loss=4.424631539173472, w0=73.24155999999999, w1=13.522884473281467\n",
      "SubGD iter. 491/499: loss=4.424631537507717, w0=73.24155999999999, w1=13.522850326132911\n",
      "SubGD iter. 492/499: loss=4.424631535841964, w0=73.24155999999999, w1=13.522816178984355\n",
      "SubGD iter. 493/499: loss=4.42463153417621, w0=73.24155999999999, w1=13.5227820318358\n",
      "SubGD iter. 494/499: loss=4.424631532510456, w0=73.24155999999999, w1=13.522747884687243\n",
      "SubGD iter. 495/499: loss=4.424631530844702, w0=73.24155999999999, w1=13.522713737538687\n",
      "SubGD iter. 496/499: loss=4.424631530657551, w0=73.2417, w1=13.522936647164551\n",
      "SubGD iter. 497/499: loss=4.424631540668822, w0=73.24155999999999, w1=13.522923857863084\n",
      "SubGD iter. 498/499: loss=4.424631539428963, w0=73.24155999999999, w1=13.522889710714528\n",
      "SubGD iter. 499/499: loss=4.424631537763208, w0=73.24155999999999, w1=13.522855563565972\n",
      "SubGD: execution time=0.556 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subgd_losses, subgd_ws = subgradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2166e503f94a47b2bc79404867db1e66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subgd_losses,\n",
    "        subgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Subgradient Descent\n",
    "\n",
    "**NB** for the computation of the subgradient you can reuse the `compute_subgradient` method that you implemented above, just making sure that you pass in a minibatch as opposed to the full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_subgradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Compute a stochastic subgradient at w from a data sample batch of size B, where B < N, and their corresponding labels.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(B, )\n",
    "        tx: numpy array of shape=(B,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic subgradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SubSGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubSGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubSGD\n",
    "    \"\"\"\n",
    "\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: implement stochastic subgradient descent.\n",
    "        # ***************************************************\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size):\n",
    "            # Calculer le gradient stochastique pour ce mini-batch\n",
    "            subgradient = compute_subgradient_mae(minibatch_y, minibatch_tx, w)\n",
    "\n",
    "            # Mettre à jour les poids avec le gradient stochastique\n",
    "            w = w - gamma * subgradient\n",
    "\n",
    "            # Calculer la perte pour ce mini-batch\n",
    "            loss = compute_loss_MAE(minibatch_y, minibatch_tx, w)\n",
    "            \n",
    "            # Stocker les poids et les pertes\n",
    "        ws.append(w.copy())\n",
    "        losses.append(loss)\n",
    "\n",
    "        print(\n",
    "            \"SubSGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubSGD iter. 0/499: loss=77.31549480507034, w0=0.7, w1=0.2069325645935772\n",
      "SubSGD iter. 1/499: loss=52.95414651552714, w0=1.4, w1=-0.7128856962687439\n",
      "SubSGD iter. 2/499: loss=54.72004532087549, w0=2.0999999999999996, w1=-1.3801552116303837\n",
      "SubSGD iter. 3/499: loss=49.456736911346304, w0=2.8, w1=-2.4956852400358986\n",
      "SubSGD iter. 4/499: loss=83.70344883762756, w0=3.5, w1=-1.8844758279972098\n",
      "SubSGD iter. 5/499: loss=93.52485585826686, w0=4.2, w1=-0.5827111369553704\n",
      "SubSGD iter. 6/499: loss=69.37860999349635, w0=4.9, w1=-0.2633372020742813\n",
      "SubSGD iter. 7/499: loss=40.99495358481397, w0=5.6000000000000005, w1=-1.3131241545905392\n",
      "SubSGD iter. 8/499: loss=35.17375045253651, w0=6.300000000000001, w1=-2.801502987522138\n",
      "SubSGD iter. 9/499: loss=50.825732484175866, w0=7.000000000000001, w1=-3.6257939425906245\n",
      "SubSGD iter. 10/499: loss=82.9727959876111, w0=7.700000000000001, w1=-3.0825525564697243\n",
      "SubSGD iter. 11/499: loss=53.80996669637149, w0=8.4, w1=-3.8270876347986835\n",
      "SubSGD iter. 12/499: loss=22.982218407825833, w0=9.1, w1=-5.489056534956653\n",
      "SubSGD iter. 13/499: loss=90.30722064458075, w0=9.799999999999999, w1=-4.571665906121989\n",
      "SubSGD iter. 14/499: loss=48.67280122929438, w0=10.499999999999998, w1=-5.050158447189361\n",
      "SubSGD iter. 15/499: loss=83.53006023094935, w0=11.199999999999998, w1=-4.506069818633412\n",
      "SubSGD iter. 16/499: loss=86.49838321689883, w0=11.899999999999997, w1=-3.4654457249287756\n",
      "SubSGD iter. 17/499: loss=67.66691879314834, w0=12.599999999999996, w1=-2.9971036522369854\n",
      "SubSGD iter. 18/499: loss=52.76135647998151, w0=13.299999999999995, w1=-3.742011553937262\n",
      "SubSGD iter. 19/499: loss=67.5148059581623, w0=13.999999999999995, w1=-3.351863459580399\n",
      "SubSGD iter. 20/499: loss=77.65663246455905, w0=14.699999999999994, w1=-2.430181134495098\n",
      "SubSGD iter. 21/499: loss=34.7475766149185, w0=15.399999999999993, w1=-3.0781496094161573\n",
      "SubSGD iter. 22/499: loss=69.40364293730167, w0=16.099999999999994, w1=-2.942029362111653\n",
      "SubSGD iter. 23/499: loss=77.9769016762028, w0=16.799999999999994, w1=-1.7390349015565933\n",
      "SubSGD iter. 24/499: loss=62.27639654353911, w0=17.499999999999993, w1=-1.6391884333772813\n",
      "SubSGD iter. 25/499: loss=25.201924965338026, w0=18.199999999999992, w1=-2.9575783624099055\n",
      "SubSGD iter. 26/499: loss=75.75704062425964, w0=18.89999999999999, w1=-2.245683549151987\n",
      "SubSGD iter. 27/499: loss=40.49992243836543, w0=19.59999999999999, w1=-2.801366647742043\n",
      "SubSGD iter. 28/499: loss=43.67231753342629, w0=20.29999999999999, w1=-2.9674243262109843\n",
      "SubSGD iter. 29/499: loss=49.93501349340477, w0=20.99999999999999, w1=-3.322014360721159\n",
      "SubSGD iter. 30/499: loss=73.03766928186226, w0=21.69999999999999, w1=-2.6368044810654503\n",
      "SubSGD iter. 31/499: loss=60.57491460216658, w0=22.399999999999988, w1=-2.4568548288437406\n",
      "SubSGD iter. 32/499: loss=38.439278501082676, w0=23.099999999999987, w1=-2.82436881933235\n",
      "SubSGD iter. 33/499: loss=59.381868009378024, w0=23.799999999999986, w1=-2.9035429368927286\n",
      "SubSGD iter. 34/499: loss=46.115498747994806, w0=24.499999999999986, w1=-3.171317467633421\n",
      "SubSGD iter. 35/499: loss=46.05722361206942, w0=25.199999999999985, w1=-2.8078152437307824\n",
      "SubSGD iter. 36/499: loss=55.39274614402893, w0=25.899999999999984, w1=-2.5669448450757058\n",
      "SubSGD iter. 37/499: loss=44.9435264618314, w0=26.599999999999984, w1=-2.430085686440595\n",
      "SubSGD iter. 38/499: loss=22.6665364738109, w0=27.299999999999983, w1=-3.202922496782953\n",
      "SubSGD iter. 39/499: loss=52.99206755329357, w0=27.999999999999982, w1=-2.7640189528732435\n",
      "SubSGD iter. 40/499: loss=56.82892388829279, w0=28.69999999999998, w1=-2.051435603166366\n",
      "SubSGD iter. 41/499: loss=34.4287920349291, w0=29.39999999999998, w1=-2.2896755921759993\n",
      "SubSGD iter. 42/499: loss=32.10146625323058, w0=30.09999999999998, w1=-2.838211952062345\n",
      "SubSGD iter. 43/499: loss=45.342584501988334, w0=30.79999999999998, w1=-2.5086190866558473\n",
      "SubSGD iter. 44/499: loss=52.92251579125363, w0=31.49999999999998, w1=-2.0305523635422733\n",
      "SubSGD iter. 45/499: loss=43.06508798630099, w0=32.19999999999998, w1=-2.2625134957320676\n",
      "SubSGD iter. 46/499: loss=63.620721217774204, w0=32.899999999999984, w1=-1.3077726248741677\n",
      "SubSGD iter. 47/499: loss=41.661503486124595, w0=33.59999999999999, w1=-1.2345438193070069\n",
      "SubSGD iter. 48/499: loss=50.528537506511626, w0=34.29999999999999, w1=-0.8678419967319804\n",
      "SubSGD iter. 49/499: loss=53.22378046855912, w0=34.99999999999999, w1=-0.34788935469847915\n",
      "SubSGD iter. 50/499: loss=57.45153368099282, w0=35.699999999999996, w1=0.31860867094170797\n",
      "SubSGD iter. 51/499: loss=22.14366714179387, w0=36.4, w1=-0.2055502080369398\n",
      "SubSGD iter. 52/499: loss=11.230235449565086, w0=37.1, w1=-1.2781647992125829\n",
      "SubSGD iter. 53/499: loss=49.65600288058638, w0=37.800000000000004, w1=-0.8401477318988682\n",
      "SubSGD iter. 54/499: loss=24.531278541702882, w0=38.50000000000001, w1=-1.286613965627151\n",
      "SubSGD iter. 55/499: loss=12.047790637025855, w0=39.20000000000001, w1=-1.9716882838382497\n",
      "SubSGD iter. 56/499: loss=57.94633363277747, w0=39.90000000000001, w1=-1.315463053234668\n",
      "SubSGD iter. 57/499: loss=40.179226905373795, w0=40.600000000000016, w1=-1.0745926545795914\n",
      "SubSGD iter. 58/499: loss=38.62348325561935, w0=41.30000000000002, w1=-0.8638262562888335\n",
      "SubSGD iter. 59/499: loss=26.477357649530788, w0=42.00000000000002, w1=-0.9206086641083371\n",
      "SubSGD iter. 60/499: loss=7.569588332363509, w0=42.700000000000024, w1=-2.0444229926082658\n",
      "SubSGD iter. 61/499: loss=38.30864169831404, w0=43.40000000000003, w1=-1.4791187009987026\n",
      "SubSGD iter. 62/499: loss=44.800995689336254, w0=44.10000000000003, w1=-0.7383039128601059\n",
      "SubSGD iter. 63/499: loss=32.615424954897186, w0=44.80000000000003, w1=-0.5488153080389501\n",
      "SubSGD iter. 64/499: loss=34.32666652049073, w0=45.500000000000036, w1=-0.4053700391319155\n",
      "SubSGD iter. 65/499: loss=20.592302064412245, w0=46.20000000000004, w1=-0.7107346847913814\n",
      "SubSGD iter. 66/499: loss=34.537513300854464, w0=46.90000000000004, w1=-0.5005290162700097\n",
      "SubSGD iter. 67/499: loss=36.000500923555265, w0=47.600000000000044, w1=0.17167004680017905\n",
      "SubSGD iter. 68/499: loss=16.17015950807471, w0=48.30000000000005, w1=-0.18114534525941384\n",
      "SubSGD iter. 69/499: loss=17.631891013387353, w0=49.00000000000005, w1=-0.46893609330878055\n",
      "SubSGD iter. 70/499: loss=44.96811229351504, w0=49.70000000000005, w1=0.6344677135454477\n",
      "SubSGD iter. 71/499: loss=24.45989289559737, w0=50.400000000000055, w1=0.6695001158552403\n",
      "SubSGD iter. 72/499: loss=7.966170388671458, w0=51.10000000000006, w1=-0.4447620589696375\n",
      "SubSGD iter. 73/499: loss=4.905212707645269, w0=51.80000000000006, w1=-1.1392643430906126\n",
      "SubSGD iter. 74/499: loss=21.0587079121361, w0=52.500000000000064, w1=-0.950098216365727\n",
      "SubSGD iter. 75/499: loss=35.837613612891985, w0=53.20000000000007, w1=-0.4884530535020367\n",
      "SubSGD iter. 76/499: loss=29.41775355535092, w0=53.90000000000007, w1=0.14723764054261324\n",
      "SubSGD iter. 77/499: loss=13.374163068559128, w0=54.60000000000007, w1=0.17670050107322072\n",
      "SubSGD iter. 78/499: loss=1.447543189359621, w0=55.300000000000075, w1=-0.5822401998794793\n",
      "SubSGD iter. 79/499: loss=8.978896851524844, w0=56.00000000000008, w1=-0.916304420382247\n",
      "SubSGD iter. 80/499: loss=9.31069185416412, w0=56.70000000000008, w1=-1.5889646129205228\n",
      "SubSGD iter. 81/499: loss=29.740451297980428, w0=57.400000000000084, w1=-1.2135678076033523\n",
      "SubSGD iter. 82/499: loss=26.293765380490285, w0=58.10000000000009, w1=-0.9709947871018276\n",
      "SubSGD iter. 83/499: loss=8.17316638788953, w0=58.80000000000009, w1=-1.3186654348007882\n",
      "SubSGD iter. 84/499: loss=42.06238075227077, w0=59.50000000000009, w1=-0.07898662055521721\n",
      "SubSGD iter. 85/499: loss=24.986945778446838, w0=60.200000000000095, w1=0.6009796541156665\n",
      "SubSGD iter. 86/499: loss=21.014406997067425, w0=60.9000000000001, w1=0.619258655746918\n",
      "SubSGD iter. 87/499: loss=28.934009240328322, w0=61.6000000000001, w1=2.145986572873108\n",
      "SubSGD iter. 88/499: loss=7.515681414948546, w0=62.300000000000104, w1=1.991217993415131\n",
      "SubSGD iter. 89/499: loss=26.266167288623492, w0=63.00000000000011, w1=2.9825487471738166\n",
      "SubSGD iter. 90/499: loss=6.531877360881538, w0=63.70000000000011, w1=2.44198286207592\n",
      "SubSGD iter. 91/499: loss=14.785704173652519, w0=64.4000000000001, w1=2.5208885979237228\n",
      "SubSGD iter. 92/499: loss=7.033309589968596, w0=65.10000000000011, w1=2.241249302742859\n",
      "SubSGD iter. 93/499: loss=2.579872687903247, w0=64.4000000000001, w1=2.7057003736875194\n",
      "SubSGD iter. 94/499: loss=8.863690839307864, w0=65.10000000000011, w1=2.743422292288064\n",
      "SubSGD iter. 95/499: loss=0.45321745063913, w0=64.4000000000001, w1=2.8562001254579275\n",
      "SubSGD iter. 96/499: loss=28.068678923469747, w0=65.10000000000011, w1=3.8697990417249\n",
      "SubSGD iter. 97/499: loss=4.903643982849772, w0=65.80000000000011, w1=3.8643585258232194\n",
      "SubSGD iter. 98/499: loss=2.266278068803679, w0=66.50000000000011, w1=3.15972060390422\n",
      "SubSGD iter. 99/499: loss=2.702957000393539, w0=67.20000000000012, w1=3.3975039601830703\n",
      "SubSGD iter. 100/499: loss=12.211532136682095, w0=67.90000000000012, w1=3.6030016923333625\n",
      "SubSGD iter. 101/499: loss=14.47928494479271, w0=68.60000000000012, w1=4.295102272133169\n",
      "SubSGD iter. 102/499: loss=9.781238495544478, w0=67.90000000000012, w1=5.398359983135318\n",
      "SubSGD iter. 103/499: loss=14.245928466683509, w0=68.60000000000012, w1=5.886530247800846\n",
      "SubSGD iter. 104/499: loss=0.9853381481929588, w0=67.90000000000012, w1=6.766899514069093\n",
      "SubSGD iter. 105/499: loss=4.855113825005162, w0=67.20000000000012, w1=6.840067324059045\n",
      "SubSGD iter. 106/499: loss=0.6953806720319875, w0=66.50000000000011, w1=6.83066014746993\n",
      "SubSGD iter. 107/499: loss=9.866553581916676, w0=67.20000000000012, w1=7.796389109593018\n",
      "SubSGD iter. 108/499: loss=13.96032627531828, w0=67.90000000000012, w1=8.529499937900354\n",
      "SubSGD iter. 109/499: loss=0.5958213671041364, w0=67.20000000000012, w1=10.114719612430568\n",
      "SubSGD iter. 110/499: loss=8.081955971971936, w0=67.90000000000012, w1=10.617030287434153\n",
      "SubSGD iter. 111/499: loss=9.157215925521228, w0=68.60000000000012, w1=11.849813930380115\n",
      "SubSGD iter. 112/499: loss=9.04295627260646, w0=69.30000000000013, w1=11.907932866667483\n",
      "SubSGD iter. 113/499: loss=4.963076050697239, w0=70.00000000000013, w1=12.313504971032346\n",
      "SubSGD iter. 114/499: loss=1.7172688254088797, w0=70.70000000000013, w1=13.370159300815487\n",
      "SubSGD iter. 115/499: loss=3.209903949116523, w0=71.40000000000013, w1=14.027228742293591\n",
      "SubSGD iter. 116/499: loss=0.15083911433491437, w0=72.10000000000014, w1=13.994075731363157\n",
      "SubSGD iter. 117/499: loss=5.2528084641252235, w0=72.80000000000014, w1=14.784832826469382\n",
      "SubSGD iter. 118/499: loss=7.740062231220719, w0=72.10000000000014, w1=14.935122455890015\n",
      "SubSGD iter. 119/499: loss=10.967123103603967, w0=71.40000000000013, w1=14.245998619025178\n",
      "SubSGD iter. 120/499: loss=0.48939366261932093, w0=72.10000000000014, w1=14.079101907260913\n",
      "SubSGD iter. 121/499: loss=0.25505405522287106, w0=72.80000000000014, w1=14.288543737483664\n",
      "SubSGD iter. 122/499: loss=3.6324971768106877, w0=72.10000000000014, w1=14.336520021339405\n",
      "SubSGD iter. 123/499: loss=0.053113172638532546, w0=72.80000000000014, w1=13.528076394543803\n",
      "SubSGD iter. 124/499: loss=0.4687530490994476, w0=73.50000000000014, w1=14.22678572608599\n",
      "SubSGD iter. 125/499: loss=5.316321907739095, w0=74.20000000000014, w1=14.659708180926419\n",
      "SubSGD iter. 126/499: loss=1.839025886245821, w0=73.50000000000014, w1=14.7959520774262\n",
      "SubSGD iter. 127/499: loss=7.028727421575098, w0=72.80000000000014, w1=14.375502324389725\n",
      "SubSGD iter. 128/499: loss=0.6687280003085618, w0=73.50000000000014, w1=14.23728134484782\n",
      "SubSGD iter. 129/499: loss=0.3240801853520594, w0=72.80000000000014, w1=13.191341500143237\n",
      "SubSGD iter. 130/499: loss=1.5560237931892402, w0=72.10000000000014, w1=12.621339646360394\n",
      "SubSGD iter. 131/499: loss=4.146032578755808, w0=72.80000000000014, w1=13.266792269877458\n",
      "SubSGD iter. 132/499: loss=1.2092487844524982, w0=72.10000000000014, w1=13.093360163820199\n",
      "SubSGD iter. 133/499: loss=1.1580087484049244, w0=71.40000000000013, w1=13.776723913139042\n",
      "SubSGD iter. 134/499: loss=0.6170643727001703, w0=72.10000000000014, w1=13.637152736757876\n",
      "SubSGD iter. 135/499: loss=6.40533757076291, w0=72.80000000000014, w1=13.339138461925184\n",
      "SubSGD iter. 136/499: loss=5.870772337063784, w0=72.10000000000014, w1=13.296111190354786\n",
      "SubSGD iter. 137/499: loss=0.12620427348294072, w0=72.80000000000014, w1=12.903615281391284\n",
      "SubSGD iter. 138/499: loss=2.6159196132453246, w0=73.50000000000014, w1=13.581789190853852\n",
      "SubSGD iter. 139/499: loss=3.773241889440939, w0=72.80000000000014, w1=13.385998876512994\n",
      "SubSGD iter. 140/499: loss=3.1739644870416726, w0=73.50000000000014, w1=13.624992753887067\n",
      "SubSGD iter. 141/499: loss=0.5441411196566861, w0=72.80000000000014, w1=13.483813271662717\n",
      "SubSGD iter. 142/499: loss=4.970157031343682, w0=73.50000000000014, w1=12.535212508971465\n",
      "SubSGD iter. 143/499: loss=9.173192060600584, w0=72.80000000000014, w1=12.19777534230489\n",
      "SubSGD iter. 144/499: loss=6.221906502036731, w0=72.10000000000014, w1=12.235456960975995\n",
      "SubSGD iter. 145/499: loss=2.1120072385954956, w0=72.80000000000014, w1=11.314841716198112\n",
      "SubSGD iter. 146/499: loss=1.3256176014537715, w0=73.50000000000014, w1=10.64367471263472\n",
      "SubSGD iter. 147/499: loss=4.280646426730485, w0=72.80000000000014, w1=11.00331336652953\n",
      "SubSGD iter. 148/499: loss=0.7041243762657814, w0=73.50000000000014, w1=11.110461853061233\n",
      "SubSGD iter. 149/499: loss=9.000537481216043, w0=72.80000000000014, w1=11.135833289299288\n",
      "SubSGD iter. 150/499: loss=0.014767028802062043, w0=72.10000000000014, w1=11.839541492147823\n",
      "SubSGD iter. 151/499: loss=2.1156032389442814, w0=71.40000000000013, w1=12.67560127353465\n",
      "SubSGD iter. 152/499: loss=17.233747156674255, w0=72.10000000000014, w1=13.19450731921781\n",
      "SubSGD iter. 153/499: loss=4.306792857625943, w0=71.40000000000013, w1=13.990630989058984\n",
      "SubSGD iter. 154/499: loss=4.215022742609236, w0=70.70000000000013, w1=12.805420401400108\n",
      "SubSGD iter. 155/499: loss=9.099393993307942, w0=71.40000000000013, w1=12.220028237781676\n",
      "SubSGD iter. 156/499: loss=0.8346602381586337, w0=70.70000000000013, w1=12.77124022047965\n",
      "SubSGD iter. 157/499: loss=9.813652396248415, w0=71.40000000000013, w1=12.299514336818143\n",
      "SubSGD iter. 158/499: loss=0.765416935336205, w0=70.70000000000013, w1=13.435462600373025\n",
      "SubSGD iter. 159/499: loss=4.167384145026844, w0=71.40000000000013, w1=12.424260819008712\n",
      "SubSGD iter. 160/499: loss=5.293114611527756, w0=72.10000000000014, w1=12.078019494870437\n",
      "SubSGD iter. 161/499: loss=3.795377848754484, w0=71.40000000000013, w1=11.776410645719903\n",
      "SubSGD iter. 162/499: loss=9.329095989752588, w0=72.10000000000014, w1=12.097660321466247\n",
      "SubSGD iter. 163/499: loss=9.449659603587861, w0=72.80000000000014, w1=12.368571310253147\n",
      "SubSGD iter. 164/499: loss=6.6643240718236285, w0=73.50000000000014, w1=12.080194152944932\n",
      "SubSGD iter. 165/499: loss=0.5467107256840507, w0=74.20000000000014, w1=12.411239483838003\n",
      "SubSGD iter. 166/499: loss=0.4958689108736394, w0=73.50000000000014, w1=11.961673266948724\n",
      "SubSGD iter. 167/499: loss=2.771479391158266, w0=72.80000000000014, w1=13.2059473234448\n",
      "SubSGD iter. 168/499: loss=3.377121918341089, w0=72.10000000000014, w1=13.496455209120368\n",
      "SubSGD iter. 169/499: loss=1.3856008685507675, w0=72.80000000000014, w1=13.065567936855233\n",
      "SubSGD iter. 170/499: loss=1.2275378084144961, w0=73.50000000000014, w1=13.450647062517145\n",
      "SubSGD iter. 171/499: loss=4.590528296576011, w0=72.80000000000014, w1=14.503582811437422\n",
      "SubSGD iter. 172/499: loss=2.7356250675109237, w0=72.10000000000014, w1=13.058691856984332\n",
      "SubSGD iter. 173/499: loss=1.3617777405393952, w0=72.80000000000014, w1=14.099585019530831\n",
      "SubSGD iter. 174/499: loss=7.671253093777921, w0=73.50000000000014, w1=13.664010385216823\n",
      "SubSGD iter. 175/499: loss=3.974879269624765, w0=72.80000000000014, w1=13.902592121653331\n",
      "SubSGD iter. 176/499: loss=12.812193085003202, w0=73.50000000000014, w1=14.149682962170996\n",
      "SubSGD iter. 177/499: loss=4.693090890403724, w0=74.20000000000014, w1=14.721100571800148\n",
      "SubSGD iter. 178/499: loss=3.00599498668511, w0=73.50000000000014, w1=13.430162668508077\n",
      "SubSGD iter. 179/499: loss=6.604452181393967, w0=72.80000000000014, w1=13.456586595209\n",
      "SubSGD iter. 180/499: loss=7.119672241338108, w0=73.50000000000014, w1=13.914868641427628\n",
      "SubSGD iter. 181/499: loss=0.965960458508178, w0=74.20000000000014, w1=14.40621380065309\n",
      "SubSGD iter. 182/499: loss=4.095519256348865, w0=73.50000000000014, w1=14.635957220183599\n",
      "SubSGD iter. 183/499: loss=2.2551284363399304, w0=74.20000000000014, w1=15.818598842863524\n",
      "SubSGD iter. 184/499: loss=1.3511129009357177, w0=74.90000000000015, w1=15.033809250000413\n",
      "SubSGD iter. 185/499: loss=0.7148684608977476, w0=75.60000000000015, w1=15.348903672201086\n",
      "SubSGD iter. 186/499: loss=15.927503220751468, w0=74.90000000000015, w1=15.138295988944515\n",
      "SubSGD iter. 187/499: loss=1.3647879268031602, w0=74.20000000000014, w1=15.606979907048153\n",
      "SubSGD iter. 188/499: loss=3.5569074854147686, w0=73.50000000000014, w1=16.053312065599034\n",
      "SubSGD iter. 189/499: loss=3.014044377221282, w0=72.80000000000014, w1=15.761905485059073\n",
      "SubSGD iter. 190/499: loss=0.5667694716183362, w0=72.10000000000014, w1=16.30460915651564\n",
      "SubSGD iter. 191/499: loss=7.041373908526452, w0=71.40000000000013, w1=16.16174738082973\n",
      "SubSGD iter. 192/499: loss=1.9724556346546152, w0=72.10000000000014, w1=15.610269784474244\n",
      "SubSGD iter. 193/499: loss=4.517548468731789, w0=71.40000000000013, w1=15.417604843472658\n",
      "SubSGD iter. 194/499: loss=9.394184666573693, w0=72.10000000000014, w1=15.685832759980183\n",
      "SubSGD iter. 195/499: loss=2.3225950870827745, w0=72.80000000000014, w1=15.754389969273419\n",
      "SubSGD iter. 196/499: loss=3.3137576213506037, w0=73.50000000000014, w1=15.545747535724665\n",
      "SubSGD iter. 197/499: loss=1.1709315771283997, w0=74.20000000000014, w1=16.152651494754732\n",
      "SubSGD iter. 198/499: loss=6.644894796506364, w0=74.90000000000015, w1=16.542301032688577\n",
      "SubSGD iter. 199/499: loss=3.609719962303231, w0=75.60000000000015, w1=16.39672492737042\n",
      "SubSGD iter. 200/499: loss=9.293674160860036, w0=74.90000000000015, w1=14.981145738877915\n",
      "SubSGD iter. 201/499: loss=2.0371798850005547, w0=75.60000000000015, w1=15.208873604047707\n",
      "SubSGD iter. 202/499: loss=0.3382333459542224, w0=74.90000000000015, w1=15.56713544098637\n",
      "SubSGD iter. 203/499: loss=4.742118979248282, w0=74.20000000000014, w1=15.88358443204121\n",
      "SubSGD iter. 204/499: loss=7.147170107332222, w0=73.50000000000014, w1=16.710103346331675\n",
      "SubSGD iter. 205/499: loss=0.33431373170189715, w0=72.80000000000014, w1=15.904196691839081\n",
      "SubSGD iter. 206/499: loss=8.930569533934047, w0=72.10000000000014, w1=15.71274051765407\n",
      "SubSGD iter. 207/499: loss=2.531970208428419, w0=71.40000000000013, w1=15.308259334474048\n",
      "SubSGD iter. 208/499: loss=2.5805048460664324, w0=72.10000000000014, w1=15.955739569345182\n",
      "SubSGD iter. 209/499: loss=3.26361238597152, w0=71.40000000000013, w1=16.18014832904146\n",
      "SubSGD iter. 210/499: loss=1.1258310050999967, w0=72.10000000000014, w1=16.257738021288993\n",
      "SubSGD iter. 211/499: loss=16.181280914680528, w0=72.80000000000014, w1=15.88767981741374\n",
      "SubSGD iter. 212/499: loss=1.9172932008169994, w0=73.50000000000014, w1=16.29429810429988\n",
      "SubSGD iter. 213/499: loss=4.614857342218031, w0=72.80000000000014, w1=16.137979232025593\n",
      "SubSGD iter. 214/499: loss=0.17421911597742223, w0=73.50000000000014, w1=16.92230680193786\n",
      "SubSGD iter. 215/499: loss=5.23952595339496, w0=72.80000000000014, w1=16.531613254705263\n",
      "SubSGD iter. 216/499: loss=0.8638452676528985, w0=73.50000000000014, w1=15.980135658349777\n",
      "SubSGD iter. 217/499: loss=8.082581134851523, w0=72.80000000000014, w1=15.95606321826882\n",
      "SubSGD iter. 218/499: loss=2.3920012417206067, w0=72.10000000000014, w1=16.106577784404266\n",
      "SubSGD iter. 219/499: loss=0.3800708962432111, w0=72.80000000000014, w1=15.896241811986535\n",
      "SubSGD iter. 220/499: loss=2.7826980936821712, w0=72.10000000000014, w1=15.861938518571325\n",
      "SubSGD iter. 221/499: loss=2.320109843565959, w0=71.40000000000013, w1=15.725072042842871\n",
      "SubSGD iter. 222/499: loss=3.481050275870672, w0=72.10000000000014, w1=16.228242361448924\n",
      "SubSGD iter. 223/499: loss=0.9766841164645967, w0=71.40000000000013, w1=16.827268195063215\n",
      "SubSGD iter. 224/499: loss=5.137467403909504, w0=72.10000000000014, w1=15.827391536188626\n",
      "SubSGD iter. 225/499: loss=1.3005382543488864, w0=72.80000000000014, w1=14.69458613415744\n",
      "SubSGD iter. 226/499: loss=7.293990421407372, w0=72.10000000000014, w1=15.007437334123127\n",
      "SubSGD iter. 227/499: loss=3.058069234153365, w0=72.80000000000014, w1=16.116198827191475\n",
      "SubSGD iter. 228/499: loss=0.3464913115277568, w0=73.50000000000014, w1=15.402104185273718\n",
      "SubSGD iter. 229/499: loss=0.0799299810102383, w0=72.80000000000014, w1=14.601041223064335\n",
      "SubSGD iter. 230/499: loss=0.4393957063129861, w0=73.50000000000014, w1=13.74917595690453\n",
      "SubSGD iter. 231/499: loss=2.21440217056886, w0=74.20000000000014, w1=13.608542831490778\n",
      "SubSGD iter. 232/499: loss=4.669041923563036, w0=74.90000000000015, w1=12.813807183582428\n",
      "SubSGD iter. 233/499: loss=5.143152220230945, w0=74.20000000000014, w1=11.974187342860034\n",
      "SubSGD iter. 234/499: loss=6.2617856670176195, w0=73.50000000000014, w1=10.57090439584211\n",
      "SubSGD iter. 235/499: loss=6.216684092191301, w0=74.20000000000014, w1=11.629533805990034\n",
      "SubSGD iter. 236/499: loss=3.7129631798501634, w0=73.50000000000014, w1=12.637830381390922\n",
      "SubSGD iter. 237/499: loss=2.690814380404717, w0=72.80000000000014, w1=12.827923558273945\n",
      "SubSGD iter. 238/499: loss=3.257489760189742, w0=72.10000000000014, w1=13.263431316377362\n",
      "SubSGD iter. 239/499: loss=10.830164212945519, w0=71.40000000000013, w1=12.1734608065666\n",
      "SubSGD iter. 240/499: loss=6.8563472630169855, w0=72.10000000000014, w1=12.14008293427783\n",
      "SubSGD iter. 241/499: loss=2.2142589851371923, w0=72.80000000000014, w1=12.030610482848095\n",
      "SubSGD iter. 242/499: loss=3.513477640246819, w0=72.10000000000014, w1=11.927344587291865\n",
      "SubSGD iter. 243/499: loss=0.19721116479637146, w0=72.80000000000014, w1=13.32389829747333\n",
      "SubSGD iter. 244/499: loss=6.588223399903512, w0=72.10000000000014, w1=13.277662287416359\n",
      "SubSGD iter. 245/499: loss=8.303981198805744, w0=72.80000000000014, w1=13.623421335662197\n",
      "SubSGD iter. 246/499: loss=4.325021192440538, w0=72.10000000000014, w1=14.798546357302287\n",
      "SubSGD iter. 247/499: loss=0.5972324183541815, w0=72.80000000000014, w1=14.075972484571889\n",
      "SubSGD iter. 248/499: loss=9.464926038642076, w0=73.50000000000014, w1=14.168804651854243\n",
      "SubSGD iter. 249/499: loss=0.6212390055271015, w0=74.20000000000014, w1=13.796728813560426\n",
      "SubSGD iter. 250/499: loss=2.0984804891583764, w0=74.90000000000015, w1=12.964072272220706\n",
      "SubSGD iter. 251/499: loss=1.0750389540619807, w0=75.60000000000015, w1=13.982522157326637\n",
      "SubSGD iter. 252/499: loss=4.60235511332597, w0=74.90000000000015, w1=13.732525608135388\n",
      "SubSGD iter. 253/499: loss=2.7461292253590983, w0=74.20000000000014, w1=12.235639453831528\n",
      "SubSGD iter. 254/499: loss=0.5729071582547505, w0=73.50000000000014, w1=12.334273994492317\n",
      "SubSGD iter. 255/499: loss=5.670261220565102, w0=72.80000000000014, w1=12.709322795159299\n",
      "SubSGD iter. 256/499: loss=2.363341301768237, w0=72.10000000000014, w1=13.596504371596932\n",
      "SubSGD iter. 257/499: loss=0.622232684729056, w0=72.80000000000014, w1=14.140253186447463\n",
      "SubSGD iter. 258/499: loss=2.8243336953187423, w0=73.50000000000014, w1=14.198380450541004\n",
      "SubSGD iter. 259/499: loss=1.6192910731621453, w0=74.20000000000014, w1=13.625148322125238\n",
      "SubSGD iter. 260/499: loss=3.834162200162922, w0=73.50000000000014, w1=13.279043330735476\n",
      "SubSGD iter. 261/499: loss=0.7591114233175205, w0=72.80000000000014, w1=13.574803525781952\n",
      "SubSGD iter. 262/499: loss=0.1981040544163477, w0=72.10000000000014, w1=13.184655431425089\n",
      "SubSGD iter. 263/499: loss=2.0689959023106184, w0=72.80000000000014, w1=14.405097286158213\n",
      "SubSGD iter. 264/499: loss=6.135646198332111, w0=72.10000000000014, w1=14.894793009266438\n",
      "SubSGD iter. 265/499: loss=8.251654687537055, w0=71.40000000000013, w1=13.848578539380517\n",
      "SubSGD iter. 266/499: loss=1.5666002407298265, w0=72.10000000000014, w1=13.872593366277503\n",
      "SubSGD iter. 267/499: loss=10.828174081600771, w0=72.80000000000014, w1=13.111224445205542\n",
      "SubSGD iter. 268/499: loss=3.048465755744658, w0=72.10000000000014, w1=13.510278904619428\n",
      "SubSGD iter. 269/499: loss=3.3601971968419306, w0=71.40000000000013, w1=14.296918600262217\n",
      "SubSGD iter. 270/499: loss=1.7578061216314254, w0=72.10000000000014, w1=13.448698170404336\n",
      "SubSGD iter. 271/499: loss=0.37525768659578773, w0=71.40000000000013, w1=12.997704245271974\n",
      "SubSGD iter. 272/499: loss=1.301309555553182, w0=70.70000000000013, w1=13.756644946224673\n",
      "SubSGD iter. 273/499: loss=0.9754156729305521, w0=71.40000000000013, w1=13.481036066964574\n",
      "SubSGD iter. 274/499: loss=0.1622938427083085, w0=70.70000000000013, w1=12.428316432217407\n",
      "SubSGD iter. 275/499: loss=0.16059126465422224, w0=71.40000000000013, w1=11.887141764059512\n",
      "SubSGD iter. 276/499: loss=3.1644111634420398, w0=70.70000000000013, w1=12.053199442528454\n",
      "SubSGD iter. 277/499: loss=3.01826830291904, w0=71.40000000000013, w1=12.663828750543233\n",
      "SubSGD iter. 278/499: loss=7.599406977998683, w0=72.10000000000014, w1=13.258132749753788\n",
      "SubSGD iter. 279/499: loss=2.1761937512700342, w0=72.80000000000014, w1=13.562000612990644\n",
      "SubSGD iter. 280/499: loss=4.348129980407521, w0=73.50000000000014, w1=13.749261736481094\n",
      "SubSGD iter. 281/499: loss=1.0588304559224753, w0=74.20000000000014, w1=12.639951192961888\n",
      "SubSGD iter. 282/499: loss=0.3306460346347677, w0=73.50000000000014, w1=12.356991070544186\n",
      "SubSGD iter. 283/499: loss=0.8227185917222926, w0=74.20000000000014, w1=11.883530437994542\n",
      "SubSGD iter. 284/499: loss=5.436674346284278, w0=74.90000000000015, w1=11.586002252989477\n",
      "SubSGD iter. 285/499: loss=2.105893451554337, w0=75.60000000000015, w1=12.599126410453087\n",
      "SubSGD iter. 286/499: loss=6.606920557900892, w0=74.90000000000015, w1=12.171583290419433\n",
      "SubSGD iter. 287/499: loss=5.165553233497434, w0=75.60000000000015, w1=12.403149056322496\n",
      "SubSGD iter. 288/499: loss=6.012561659361808, w0=74.90000000000015, w1=12.930173453940647\n",
      "SubSGD iter. 289/499: loss=0.3566064330609038, w0=74.20000000000014, w1=12.470406277457688\n",
      "SubSGD iter. 290/499: loss=11.228907161061088, w0=73.50000000000014, w1=12.851707148754096\n",
      "SubSGD iter. 291/499: loss=2.997040645942718, w0=74.20000000000014, w1=13.758816523361535\n",
      "SubSGD iter. 292/499: loss=0.12451540322642529, w0=73.50000000000014, w1=13.132117028084696\n",
      "SubSGD iter. 293/499: loss=0.9186909818851632, w0=72.80000000000014, w1=12.934032300713985\n",
      "SubSGD iter. 294/499: loss=5.917481350773727, w0=73.50000000000014, w1=12.605308223618257\n",
      "SubSGD iter. 295/499: loss=7.747840361919572, w0=74.20000000000014, w1=12.775576320268245\n",
      "SubSGD iter. 296/499: loss=7.55684133098724, w0=73.50000000000014, w1=12.81377213099398\n",
      "SubSGD iter. 297/499: loss=2.269763162495437, w0=72.80000000000014, w1=12.824872525007898\n",
      "SubSGD iter. 298/499: loss=0.43979616951313005, w0=73.50000000000014, w1=12.188024925817126\n",
      "SubSGD iter. 299/499: loss=5.24917432926388, w0=74.20000000000014, w1=11.459071865204438\n",
      "SubSGD iter. 300/499: loss=8.232895661238715, w0=73.50000000000014, w1=11.908867989390595\n",
      "SubSGD iter. 301/499: loss=2.585577911951546, w0=74.20000000000014, w1=12.032609979655845\n",
      "SubSGD iter. 302/499: loss=1.1452389417392794, w0=73.50000000000014, w1=12.764712614447623\n",
      "SubSGD iter. 303/499: loss=1.0262477603022973, w0=72.80000000000014, w1=12.210628329617336\n",
      "SubSGD iter. 304/499: loss=0.31241840127474063, w0=73.50000000000014, w1=11.944305725915514\n",
      "SubSGD iter. 305/499: loss=3.5727809916525217, w0=72.80000000000014, w1=12.489875267950474\n",
      "SubSGD iter. 306/499: loss=3.1224075283005703, w0=72.10000000000014, w1=12.776717679671794\n",
      "SubSGD iter. 307/499: loss=0.3736022088826445, w0=71.40000000000013, w1=12.67678078680636\n",
      "SubSGD iter. 308/499: loss=1.7764167665361157, w0=70.70000000000013, w1=11.580913799140005\n",
      "SubSGD iter. 309/499: loss=2.3100283703455204, w0=70.00000000000013, w1=11.17917129202316\n",
      "SubSGD iter. 310/499: loss=2.560954942214302, w0=70.70000000000013, w1=10.454665267001406\n",
      "SubSGD iter. 311/499: loss=9.44864728089729, w0=71.40000000000013, w1=11.238962192652512\n",
      "SubSGD iter. 312/499: loss=0.1216298877516806, w0=72.10000000000014, w1=11.5685790061624\n",
      "SubSGD iter. 313/499: loss=4.149594217220191, w0=72.80000000000014, w1=10.318899232808738\n",
      "SubSGD iter. 314/499: loss=2.962908255756183, w0=72.10000000000014, w1=10.939621553668749\n",
      "SubSGD iter. 315/499: loss=3.858766804243487, w0=72.80000000000014, w1=10.593380229530474\n",
      "SubSGD iter. 316/499: loss=7.151025588637509, w0=73.50000000000014, w1=11.081912569323457\n",
      "SubSGD iter. 317/499: loss=3.7587725449301246, w0=72.80000000000014, w1=11.027869643729241\n",
      "SubSGD iter. 318/499: loss=0.7117581168055693, w0=73.50000000000014, w1=11.498290124654378\n",
      "SubSGD iter. 319/499: loss=3.4053341203093908, w0=74.20000000000014, w1=12.981755311732288\n",
      "SubSGD iter. 320/499: loss=0.6565976431422342, w0=73.50000000000014, w1=12.061775638356737\n",
      "SubSGD iter. 321/499: loss=3.370667372347299, w0=72.80000000000014, w1=11.789663117743839\n",
      "SubSGD iter. 322/499: loss=4.429209148683412, w0=73.50000000000014, w1=12.169835535556103\n",
      "SubSGD iter. 323/499: loss=2.7422514313780653, w0=74.20000000000014, w1=12.100705976271726\n",
      "SubSGD iter. 324/499: loss=0.4114550224410749, w0=73.50000000000014, w1=12.232604160021113\n",
      "SubSGD iter. 325/499: loss=1.5534287306962113, w0=72.80000000000014, w1=11.191556184789293\n",
      "SubSGD iter. 326/499: loss=0.08492640475536462, w0=72.10000000000014, w1=10.771920345515447\n",
      "SubSGD iter. 327/499: loss=0.21542890274317728, w0=71.40000000000013, w1=11.745130037467545\n",
      "SubSGD iter. 328/499: loss=2.632562158655702, w0=72.10000000000014, w1=10.331241563570448\n",
      "SubSGD iter. 329/499: loss=1.0784234963051347, w0=72.80000000000014, w1=10.242671601011622\n",
      "SubSGD iter. 330/499: loss=8.279845202430934, w0=73.50000000000014, w1=10.231114138193298\n",
      "SubSGD iter. 331/499: loss=4.28853363872021, w0=72.80000000000014, w1=9.767482044683135\n",
      "SubSGD iter. 332/499: loss=1.9058486132487857, w0=72.10000000000014, w1=11.040985251566594\n",
      "SubSGD iter. 333/499: loss=3.708003537704009, w0=71.40000000000013, w1=11.732957456258976\n",
      "SubSGD iter. 334/499: loss=0.363801873619785, w0=70.70000000000013, w1=13.118699342817973\n",
      "SubSGD iter. 335/499: loss=11.39496071993355, w0=71.40000000000013, w1=12.507578760578738\n",
      "SubSGD iter. 336/499: loss=1.7379815662203413, w0=70.70000000000013, w1=13.099314115578297\n",
      "SubSGD iter. 337/499: loss=1.2720372603344146, w0=71.40000000000013, w1=12.144923790048978\n",
      "SubSGD iter. 338/499: loss=1.4962285630466852, w0=72.10000000000014, w1=12.262209618057971\n",
      "SubSGD iter. 339/499: loss=5.822183780208121, w0=72.80000000000014, w1=12.047253890159903\n",
      "SubSGD iter. 340/499: loss=0.3836165631912394, w0=73.50000000000014, w1=11.623502052485017\n",
      "SubSGD iter. 341/499: loss=1.8357376357178126, w0=74.20000000000014, w1=10.382886229663985\n",
      "SubSGD iter. 342/499: loss=0.648280990102073, w0=73.50000000000014, w1=11.324677056106259\n",
      "SubSGD iter. 343/499: loss=2.853208206233191, w0=74.20000000000014, w1=12.270999915806406\n",
      "SubSGD iter. 344/499: loss=0.678435342691941, w0=74.90000000000015, w1=13.577650049743282\n",
      "SubSGD iter. 345/499: loss=0.21475543074637926, w0=74.20000000000014, w1=14.565810412005806\n",
      "SubSGD iter. 346/499: loss=1.6428302128638492, w0=73.50000000000014, w1=15.522607223031397\n",
      "SubSGD iter. 347/499: loss=2.713562529652563, w0=72.80000000000014, w1=16.001164363434576\n",
      "SubSGD iter. 348/499: loss=0.03340729747118587, w0=72.10000000000014, w1=15.382182439551249\n",
      "SubSGD iter. 349/499: loss=0.5630009815025119, w0=72.80000000000014, w1=15.75113603273911\n",
      "SubSGD iter. 350/499: loss=0.7169560247590709, w0=73.50000000000014, w1=15.434252536025605\n",
      "SubSGD iter. 351/499: loss=5.888857139322937, w0=74.20000000000014, w1=15.673830287230242\n",
      "SubSGD iter. 352/499: loss=5.762440804412236, w0=73.50000000000014, w1=14.578525175951587\n",
      "SubSGD iter. 353/499: loss=1.9322404964526427, w0=72.80000000000014, w1=14.948327189870875\n",
      "SubSGD iter. 354/499: loss=1.3326977301605751, w0=73.50000000000014, w1=15.075614346260103\n",
      "SubSGD iter. 355/499: loss=12.149102752986167, w0=72.80000000000014, w1=15.124528351634439\n",
      "SubSGD iter. 356/499: loss=0.9536971269948111, w0=73.50000000000014, w1=16.167365155453393\n",
      "SubSGD iter. 357/499: loss=1.8135221272289783, w0=72.80000000000014, w1=15.212827091115589\n",
      "SubSGD iter. 358/499: loss=0.7284130383998644, w0=72.10000000000014, w1=14.347368871426484\n",
      "SubSGD iter. 359/499: loss=12.98575231977388, w0=72.80000000000014, w1=14.310793221836981\n",
      "SubSGD iter. 360/499: loss=2.1816693548729944, w0=72.10000000000014, w1=13.214926234170626\n",
      "SubSGD iter. 361/499: loss=0.09010306890304776, w0=72.80000000000014, w1=14.413453029241667\n",
      "SubSGD iter. 362/499: loss=2.287002468341541, w0=73.50000000000014, w1=14.014221526338758\n",
      "SubSGD iter. 363/499: loss=1.0871221411296403, w0=74.20000000000014, w1=14.292096089094933\n",
      "SubSGD iter. 364/499: loss=8.09300384742545, w0=73.50000000000014, w1=14.636813226852631\n",
      "SubSGD iter. 365/499: loss=0.5043739579863669, w0=74.20000000000014, w1=13.050253799703043\n",
      "SubSGD iter. 366/499: loss=8.221639492947702, w0=73.50000000000014, w1=12.536679469373942\n",
      "SubSGD iter. 367/499: loss=1.1090023653561403, w0=74.20000000000014, w1=10.869180131480052\n",
      "SubSGD iter. 368/499: loss=1.9944458771678484, w0=74.90000000000015, w1=12.357041970799663\n",
      "SubSGD iter. 369/499: loss=1.223171159787384, w0=74.20000000000014, w1=11.9668938764428\n",
      "SubSGD iter. 370/499: loss=7.344790576453342, w0=73.50000000000014, w1=12.714707758742364\n",
      "SubSGD iter. 371/499: loss=0.17333158851803887, w0=72.80000000000014, w1=12.519304255699234\n",
      "SubSGD iter. 372/499: loss=3.7537565133633706, w0=72.10000000000014, w1=12.567280539554975\n",
      "SubSGD iter. 373/499: loss=10.857216612156392, w0=72.80000000000014, w1=12.418370494571137\n",
      "SubSGD iter. 374/499: loss=2.905023263555279, w0=73.50000000000014, w1=12.141586522082005\n",
      "SubSGD iter. 375/499: loss=1.6200039544679612, w0=74.20000000000014, w1=13.44823665601888\n",
      "SubSGD iter. 376/499: loss=5.574385717737769, w0=73.50000000000014, w1=13.765030001713939\n",
      "SubSGD iter. 377/499: loss=1.3754637705594774, w0=74.20000000000014, w1=15.043911948729523\n",
      "SubSGD iter. 378/499: loss=10.402911950758423, w0=74.90000000000015, w1=13.11024237633941\n",
      "SubSGD iter. 379/499: loss=0.2666457664865902, w0=74.20000000000014, w1=11.95563773507637\n",
      "SubSGD iter. 380/499: loss=0.16867753440504885, w0=73.50000000000014, w1=12.412263767743095\n",
      "SubSGD iter. 381/499: loss=4.505694211794463, w0=72.80000000000014, w1=12.646042632043718\n",
      "SubSGD iter. 382/499: loss=11.138763696989038, w0=72.10000000000014, w1=12.604313039535693\n",
      "SubSGD iter. 383/499: loss=0.7445151158287189, w0=72.80000000000014, w1=12.718629307425333\n",
      "SubSGD iter. 384/499: loss=0.731453164173395, w0=72.10000000000014, w1=13.337886888091154\n",
      "SubSGD iter. 385/499: loss=2.258716015056166, w0=72.80000000000014, w1=14.084555870462884\n",
      "SubSGD iter. 386/499: loss=3.5880439525059273, w0=72.10000000000014, w1=13.605670327624368\n",
      "SubSGD iter. 387/499: loss=5.667900557676646, w0=72.80000000000014, w1=13.66047925935951\n",
      "SubSGD iter. 388/499: loss=3.738450862164548, w0=72.10000000000014, w1=13.999982543095733\n",
      "SubSGD iter. 389/499: loss=4.250387611926328, w0=72.80000000000014, w1=13.793118640671748\n",
      "SubSGD iter. 390/499: loss=0.3161153848423979, w0=72.10000000000014, w1=14.622624665118082\n",
      "SubSGD iter. 391/499: loss=1.6003895515223974, w0=71.40000000000013, w1=13.42093787611379\n",
      "SubSGD iter. 392/499: loss=0.011195713410032226, w0=70.70000000000013, w1=13.586209315695713\n",
      "SubSGD iter. 393/499: loss=1.9607898745136652, w0=70.00000000000013, w1=14.568107611035181\n",
      "SubSGD iter. 394/499: loss=1.2174972621039188, w0=70.70000000000013, w1=14.785875991531785\n",
      "SubSGD iter. 395/499: loss=1.9478424532572234, w0=71.40000000000013, w1=15.787604209807604\n",
      "SubSGD iter. 396/499: loss=6.739042216511955, w0=72.10000000000014, w1=14.591922454387884\n",
      "SubSGD iter. 397/499: loss=4.051304715764964, w0=72.80000000000014, w1=13.611826412225\n",
      "SubSGD iter. 398/499: loss=2.3696498999595406, w0=72.10000000000014, w1=14.618581533091803\n",
      "SubSGD iter. 399/499: loss=0.7340926164166746, w0=71.40000000000013, w1=14.13738681216285\n",
      "SubSGD iter. 400/499: loss=14.218236248241737, w0=72.10000000000014, w1=13.430190431008038\n",
      "SubSGD iter. 401/499: loss=10.107946334187353, w0=71.40000000000013, w1=13.41587472673143\n",
      "SubSGD iter. 402/499: loss=3.0897506218221906, w0=70.70000000000013, w1=13.204219073226213\n",
      "SubSGD iter. 403/499: loss=0.7560352961707721, w0=70.00000000000013, w1=13.835409627086113\n",
      "SubSGD iter. 404/499: loss=0.22211702760475305, w0=70.70000000000013, w1=13.49438754406123\n",
      "SubSGD iter. 405/499: loss=2.370731596099958, w0=71.40000000000013, w1=12.827968270172219\n",
      "SubSGD iter. 406/499: loss=0.10718050435147575, w0=72.10000000000014, w1=13.850688234749652\n",
      "SubSGD iter. 407/499: loss=0.519621063196297, w0=71.40000000000013, w1=14.300115574631771\n",
      "SubSGD iter. 408/499: loss=0.15794152656367544, w0=72.10000000000014, w1=14.0316738777228\n",
      "SubSGD iter. 409/499: loss=4.849993672028063, w0=72.80000000000014, w1=14.0106758479559\n",
      "SubSGD iter. 410/499: loss=4.7410801620295615, w0=73.50000000000014, w1=13.92989746424256\n",
      "SubSGD iter. 411/499: loss=2.805720510393421, w0=72.80000000000014, w1=15.027782194032358\n",
      "SubSGD iter. 412/499: loss=6.659885797605611, w0=73.50000000000014, w1=14.705176937963497\n",
      "SubSGD iter. 413/499: loss=5.54808241176729, w0=72.80000000000014, w1=15.466128703092616\n",
      "SubSGD iter. 414/499: loss=2.2756746339711214, w0=73.50000000000014, w1=16.695138006059842\n",
      "SubSGD iter. 415/499: loss=11.985607584719993, w0=72.80000000000014, w1=16.624615848274313\n",
      "SubSGD iter. 416/499: loss=2.2774400405048993, w0=72.10000000000014, w1=15.821858411061973\n",
      "SubSGD iter. 417/499: loss=0.3762334136072951, w0=71.40000000000013, w1=16.87479415998225\n",
      "SubSGD iter. 418/499: loss=0.5003723172384014, w0=72.10000000000014, w1=16.348769611437774\n",
      "SubSGD iter. 419/499: loss=2.061771777266344, w0=72.80000000000014, w1=16.083962827644598\n",
      "SubSGD iter. 420/499: loss=0.23921330457588397, w0=73.50000000000014, w1=16.290895392238173\n",
      "SubSGD iter. 421/499: loss=2.5026690897157238, w0=72.80000000000014, w1=15.344249668364437\n",
      "SubSGD iter. 422/499: loss=2.7628528665571395, w0=72.10000000000014, w1=15.53761822818822\n",
      "SubSGD iter. 423/499: loss=4.4289409595725004, w0=72.80000000000014, w1=15.150599932270723\n",
      "SubSGD iter. 424/499: loss=14.062230024022526, w0=73.50000000000014, w1=14.781113848506788\n",
      "SubSGD iter. 425/499: loss=7.10980083362017, w0=72.80000000000014, w1=14.11387732145836\n",
      "SubSGD iter. 426/499: loss=0.22492166944111602, w0=73.50000000000014, w1=14.991179761277959\n",
      "SubSGD iter. 427/499: loss=4.284340226346657, w0=72.80000000000014, w1=15.0391560451337\n",
      "SubSGD iter. 428/499: loss=1.0074143541794172, w0=73.50000000000014, w1=14.66175118060694\n",
      "SubSGD iter. 429/499: loss=0.07186835805438818, w0=74.20000000000014, w1=14.462570976437211\n",
      "SubSGD iter. 430/499: loss=8.013656358830715, w0=74.90000000000015, w1=14.555403143719566\n",
      "SubSGD iter. 431/499: loss=3.518177480835618, w0=74.20000000000014, w1=12.699261616212087\n",
      "SubSGD iter. 432/499: loss=0.11984637838823176, w0=73.50000000000014, w1=12.965942667944102\n",
      "SubSGD iter. 433/499: loss=2.887212960564895, w0=72.80000000000014, w1=13.051509167108305\n",
      "SubSGD iter. 434/499: loss=1.0921027657550866, w0=73.50000000000014, w1=13.47956601145068\n",
      "SubSGD iter. 435/499: loss=4.676259854747286, w0=72.80000000000014, w1=13.653191447533283\n",
      "SubSGD iter. 436/499: loss=0.3082855174719299, w0=73.50000000000014, w1=14.337360836789031\n",
      "SubSGD iter. 437/499: loss=0.16570678571805786, w0=72.80000000000014, w1=13.192670544969383\n",
      "SubSGD iter. 438/499: loss=7.677411889318783, w0=73.50000000000014, w1=11.99144702622746\n",
      "SubSGD iter. 439/499: loss=2.858356887260925, w0=74.20000000000014, w1=11.481111304499063\n",
      "SubSGD iter. 440/499: loss=4.185123879997626, w0=73.50000000000014, w1=12.42182348584274\n",
      "SubSGD iter. 441/499: loss=0.7308826241032804, w0=72.80000000000014, w1=13.366139240891505\n",
      "SubSGD iter. 442/499: loss=7.053397569974948, w0=72.10000000000014, w1=12.852564910562403\n",
      "SubSGD iter. 443/499: loss=0.865725880302044, w0=72.80000000000014, w1=11.988692768358913\n",
      "SubSGD iter. 444/499: loss=5.813553926298752, w0=73.50000000000014, w1=12.163954256823137\n",
      "SubSGD iter. 445/499: loss=4.605062362082293, w0=74.20000000000014, w1=12.465437325148022\n",
      "SubSGD iter. 446/499: loss=4.146161201708665, w0=73.50000000000014, w1=12.102841586985745\n",
      "SubSGD iter. 447/499: loss=4.549943580933416, w0=72.80000000000014, w1=12.126874119740844\n",
      "SubSGD iter. 448/499: loss=1.3161604873001878, w0=72.10000000000014, w1=12.999560077506057\n",
      "SubSGD iter. 449/499: loss=1.613927064112417, w0=71.40000000000013, w1=13.38650712925245\n",
      "SubSGD iter. 450/499: loss=0.36090396759820464, w0=70.70000000000013, w1=12.46221445834004\n",
      "SubSGD iter. 451/499: loss=3.405455965150857, w0=70.00000000000013, w1=14.008349016167614\n",
      "SubSGD iter. 452/499: loss=5.225499705067477, w0=70.70000000000013, w1=14.514330692331306\n",
      "SubSGD iter. 453/499: loss=1.144282818787076, w0=71.40000000000013, w1=14.971291782336387\n",
      "SubSGD iter. 454/499: loss=6.2796384320215, w0=72.10000000000014, w1=15.466417114987658\n",
      "SubSGD iter. 455/499: loss=0.5675696557997014, w0=72.80000000000014, w1=15.420450404723498\n",
      "SubSGD iter. 456/499: loss=7.977146586719556, w0=73.50000000000014, w1=14.43780012755804\n",
      "SubSGD iter. 457/499: loss=2.5099767642943362, w0=74.20000000000014, w1=14.408353470855282\n",
      "SubSGD iter. 458/499: loss=0.28359165203757186, w0=74.90000000000015, w1=15.059812112971652\n",
      "SubSGD iter. 459/499: loss=1.5344311241801307, w0=74.20000000000014, w1=13.523022681840066\n",
      "SubSGD iter. 460/499: loss=0.7186529323317785, w0=73.50000000000014, w1=14.55245652592043\n",
      "SubSGD iter. 461/499: loss=2.520463253426101, w0=72.80000000000014, w1=14.54384651209783\n",
      "SubSGD iter. 462/499: loss=1.6256128422000629, w0=72.10000000000014, w1=14.865253335960734\n",
      "SubSGD iter. 463/499: loss=2.529266891892931, w0=72.80000000000014, w1=14.881508042687805\n",
      "SubSGD iter. 464/499: loss=4.433892592047457, w0=73.50000000000014, w1=13.882758892573325\n",
      "SubSGD iter. 465/499: loss=1.4341509084107074, w0=74.20000000000014, w1=13.308980514507297\n",
      "SubSGD iter. 466/499: loss=7.309026621900799, w0=73.50000000000014, w1=13.33540444120822\n",
      "SubSGD iter. 467/499: loss=6.139495576502924, w0=72.80000000000014, w1=12.6580519289145\n",
      "SubSGD iter. 468/499: loss=2.014059991697181, w0=73.50000000000014, w1=12.4149291146795\n",
      "SubSGD iter. 469/499: loss=0.09120391995480759, w0=72.80000000000014, w1=12.244666720912234\n",
      "SubSGD iter. 470/499: loss=4.992881937344407, w0=72.10000000000014, w1=12.76625202877501\n",
      "SubSGD iter. 471/499: loss=1.15524375850908, w0=72.80000000000014, w1=12.103763428112346\n",
      "SubSGD iter. 472/499: loss=3.0008305650696414, w0=73.50000000000014, w1=10.806800273573858\n",
      "SubSGD iter. 473/499: loss=4.587159063491377, w0=72.80000000000014, w1=11.737121368363471\n",
      "SubSGD iter. 474/499: loss=1.2638846098085281, w0=73.50000000000014, w1=12.82568214591829\n",
      "SubSGD iter. 475/499: loss=0.7178333047278471, w0=74.20000000000014, w1=12.942967973927283\n",
      "SubSGD iter. 476/499: loss=1.7372570461753583, w0=74.90000000000015, w1=13.151229288405695\n",
      "SubSGD iter. 477/499: loss=3.0403533935313263, w0=74.20000000000014, w1=13.608722511659629\n",
      "SubSGD iter. 478/499: loss=0.041422802656626345, w0=74.90000000000015, w1=13.639061097796358\n",
      "SubSGD iter. 479/499: loss=1.8065838480807344, w0=74.20000000000014, w1=12.069940141478867\n",
      "SubSGD iter. 480/499: loss=9.175735022745627, w0=74.90000000000015, w1=12.58776625481398\n",
      "SubSGD iter. 481/499: loss=0.8126577756885212, w0=75.60000000000015, w1=13.607831663512089\n",
      "SubSGD iter. 482/499: loss=0.4889498861168917, w0=76.30000000000015, w1=12.871633507129346\n",
      "SubSGD iter. 483/499: loss=2.116725603698967, w0=77.00000000000016, w1=13.740141459720297\n",
      "SubSGD iter. 484/499: loss=1.910931017183728, w0=76.30000000000015, w1=13.429551981001383\n",
      "SubSGD iter. 485/499: loss=1.3721643312092624, w0=75.60000000000015, w1=12.682468096934075\n",
      "SubSGD iter. 486/499: loss=9.214215556411375, w0=74.90000000000015, w1=13.14487993725622\n",
      "SubSGD iter. 487/499: loss=3.3203778366798815, w0=75.60000000000015, w1=11.772239521911898\n",
      "SubSGD iter. 488/499: loss=13.496585938885282, w0=74.90000000000015, w1=11.95851216620861\n",
      "SubSGD iter. 489/499: loss=1.2892095595682207, w0=74.20000000000014, w1=11.157739400354552\n",
      "SubSGD iter. 490/499: loss=3.5822755289758987, w0=74.90000000000015, w1=11.65614585117473\n",
      "SubSGD iter. 491/499: loss=7.98070806448402, w0=74.20000000000014, w1=12.373438686127605\n",
      "SubSGD iter. 492/499: loss=9.18289402178732, w0=74.90000000000015, w1=12.655764883539687\n",
      "SubSGD iter. 493/499: loss=0.5027539057062853, w0=74.20000000000014, w1=12.402401391600725\n",
      "SubSGD iter. 494/499: loss=11.73476868040727, w0=73.50000000000014, w1=13.191890766819064\n",
      "SubSGD iter. 495/499: loss=7.646947557942298, w0=74.20000000000014, w1=13.518617851128578\n",
      "SubSGD iter. 496/499: loss=1.881353177644982, w0=73.50000000000014, w1=13.543810579595705\n",
      "SubSGD iter. 497/499: loss=3.9212396665651283, w0=74.20000000000014, w1=13.822788224125132\n",
      "SubSGD iter. 498/499: loss=3.2730699864168287, w0=73.50000000000014, w1=12.435638397570374\n",
      "SubSGD iter. 499/499: loss=1.7145836018363667, w0=74.20000000000014, w1=12.813536661918066\n",
      "SubSGD: execution time=0.066 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subsgd_losses, subsgd_ws = stochastic_subgradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma\n",
    ")\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubSGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbb2b643589c4422882785d84166a42a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subsgd_losses,\n",
    "        subsgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subsgd_ws)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
